---
title: 'Facebook Misinformation Study, pre-analysis replication script'
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_folding: hide
    number_sections: true
---



# Data reading
```{r paths, message = FALSE}
set.seed(60637)
source('utils.R')

dir.create(file.path('..', 'tables'), showWarnings = FALSE)
dir.create(file.path('..', 'figures'), showWarnings = FALSE)
dir.create(file.path('objects'), showWarnings = FALSE)
```




## Load Data
Load most recent download of data; the file name indicates the download date. 
```{r data, cache = TRUE}
files <- list.files('../data', 
                    pattern = '^cleaned-data.*rds$', 
                    full.names = TRUE)

(INPUT_FILENAME <- files[which.max(file.info(files)$mtime)])
df_treat <- readRDS(INPUT_FILENAME)

context_cols <- c('male', 
                  'age', 
                  'age_flag',
                  'age_check_flag',
                  'ed', 'ed_flag', 
                  'urban',
                  'rel_christian', 'rel_muslim',
                  'denom_pentecostal', 
                  'religiosity', 'religiosity_flag',
                  'locus', 'locus_flag',
                  'science', 'science_flag',
                  'dli',
                  'fb_post', 'fb_post_flag',
                  'fb_msg', 'fb_msg_flag',
                  'crt',
                  'hhi', 'hhi_flag',
                  'cash',
                  'hh', 'hh_flag',
                  'pol',
                  'cov_concern', 'cov_concern_flag',
                  'cov_efficacy', 'cov_efficacy_flag',
                  'nigeria')

demos_cols <- c('male', 
                'age', 'age_flag',
                'ed', 'ed_flag', 
                'urban', 
                'rel_none', 'rel_christian', 'rel_muslim', 'rel_traditionalist', 'rel_other', 'denom_pentecostal', 'religiosity',
                'religiosity_flag',
                'god',
                'locus', 'locus_flag',
                'science', 'science_flag',
                'dli',
                'fb_post', 'fb_post_flag',
                'fb_msg', 'fb_msg_flag',
                'crt',
                'hhi', 'hhi_flag',
                'cash',
                'hh', 'hh_flag',
                'pol',
                'cov_concern', 
                'cov_concern_flag',
                'cov_info',
                'cov_efficacy', 
                'cov_efficacy_flag')

predv_cols <- c('strat_send_false0', 'strat_send_false1', 'strat_send_false2', 
                'strat_send_true0', 'strat_send_true1', 'strat_send_true2', 
                'strat_timeline_false0', 'strat_timeline_false1', 'strat_timeline_false2', 
                'strat_timeline_true0', 'strat_timeline_true1', 'strat_timeline_true2')

# names for treatment levels
W_terms <- c('Factcheck', 'More information', 'Real information', 'Related articles', 'Control', 'Accuracy nudge', 'Deliberation nudge', 'Emotion suppression', 'Pledge', 'AfricaCheck Tips', 'Facebook Tips', 'Video training') 
WC_terms <- c('Control', W_terms) # with control

# names for respondent-level treatments
WR_terms <- c('Accuracy nudge', 'Deliberation nudge', 'Emotion suppression', 'Pledge', 'AfricaCheck tips', 'Facebook tips', 'Video training')
WCR_terms <- c('Control', WR_terms) # with control

# names for headline-level treatments
WH_terms <- c('Factcheck', 'More information', 'Real information', 'Related articles')
WCH_terms <- c('Control', WH_terms) # with control

# evaluation treatment levels
treatment_levels <- c('Control', 
                      'Headline\nFactcheck', 
                      'Headline\nRelated articles', 
                      'Respondent\nAccuracy', 
                      'Respondent\nFacebook tips', 
                      'Respondent\nOptimal')
```


```{r datasets, cache = TRUE}
df_treat <- df_treat %>% 
  mutate(Y_pre = -pre_false + 0.5 * pre_true,
         ID = 1:n())
df_eval <- df_treat[which(df_treat$batch == 5),]
df_learn <- df_treat[which(df_treat$batch<5),]
```


```{r hyperparameters, cache = TRUE}
## Hyperparameters
num_batches <- 3
# times at which the model is updated (applied in following observation)
update_times <- sapply(1:4, function(x) max(df_treat$ID[which(df_treat$batch == x)]))
num_init_draws <- update_times[1]
A <- update_times[4] # no. observations in learning split/ last model update
N <- nrow(df_learn) + nrow(df_eval)
K <- length(unique((df_learn$W)))
```

```{r data_components, cache = TRUE}
xs_learn <- as.matrix(df_learn[, c(context_cols, predv_cols)])
yobs_learn <- df_learn$Y
ws_learn <- as.numeric(df_learn$W)

xs_eval <- as.matrix(df_eval[, c(context_cols, predv_cols)])
yobs_eval <- df_eval$Y
ws_eval <- as.factor(case_when(as.numeric(df_eval$W) == 1 ~ 1, # control
                               as.numeric(df_eval$W) == 2 ~  2, # headline factcheck,
                               as.numeric(df_eval$W) == 5 ~ 3, # headline related,
                               as.numeric(df_eval$W) == 6 ~ 4, # respondent accuracy
                               as.numeric(df_eval$W) == 11 ~ 5, # facebook tips,
                               # combine optimal with small groups
                               TRUE ~ 6 # other optimal respondent (8/12)
)) 

```


# Analysis Overview

## Response
The response function is:

$$Y_i = -M_i^{\text{post-treat}} + 0.5T_i^{\text{post-treat}}$$

It is composed of $M_i^{\text{post-treat}}\in \{0,1,2,3,4 \}$, the sum of times the respondent said they would share *misinformation stimuli*, and $T_i^{\text{post-treat}}\in \{0,1,2,3,4 \}$, the sum of times the respondent said they would share *true stimuli*, both post-treatment. There are two types of each stimuli, and for each two questions about sharing: whether they would share directly on Messenger, and whether they woudld share on their timeline. 

We assign greater weight to the sharing of *misinformation* in our response function, because our primary objective is to curb the spread of misinformation, although we would like to do so at minimal cost to the sharing of true information. 

For example, response value of $Y_i = -1$ indicates that the respondent responded yes to two of the questions about sharing the misinformation stimuli and two of the questions about sharing true stimuli. 

## Treatments

Treatment is composed of a respondent-level treatment and a headline-level treatment. In the adaptive learning portion of the experiment, respondent-level treatments and headline-level treatments are implemented as separate factors, each of which has an empty baseline level that is the control. So respondents may be assigned the pure control condition, one of the respondent-level treatments but no headline-level treatment, one of the headline-level treatments but no respondent-level treatment, or one of the respondent-level treatments *and* one of the headline-level treatments. 

**Respondent-level treatments:**

* Control
* Facebook tips
* AfricaCheck tips
* Video training
* Emotion suppression
* Pledge
* Accuracy nudge
* Deliberation nudge

**Headline-level treatments**

* Control
* Related articles
* Third-party factcheck
* More information link
* Real information statement

## Response under unique treatments

For use in analysis of the adaptive design, we calculate doubly robust scores. Scores are composed of a conditional means model, which is estimated by fitting $K$ separate causal forests, and a weighting term, which here are inverse probability weights using known probability of treatment assignment. 

$$
\Gamma_{i,w} = \mu_{w}(X_{i}) + 1 \{W_i = w \} \xi_w(X_i)(Y_{i} - \mu_w(X_i))
$$

$$
\mu_{w}(x)  = \textrm{E}[Y_i(w) | X_i = x]
$$

$$
\xi^{IPW}_w(X_i) = \frac{ 1 }{\Pr[W_i = w|X_i = x]}
$$

## Calculating scores

```{r factor_levels, cache=TRUE}
# For learning split only
df_learn['treatment_r'] <- relevel(as.factor(gsub('H_.*_|R_', '', df_learn$W)),
                                   ref = 'control')
df_learn['treatment_h'] <- relevel(as.factor(gsub('H_|_R_.*', '', df_learn$W)),
                                   ref = 'control')

W_levels <- levels(df_learn$W)
# Decompose treatment levels into separate factors
WH_levels <- unique(sub('_R_.*', '', W_levels))
WR_levels <- unique(sub('H_[a-z]*_*[a-z]*_*', '', W_levels))

# Identify treatment locations where each respective factor level is represented
WH_idx <- lapply(WH_levels, function(x) grep(x, W_levels))
WR_idx <- lapply(WR_levels, function(x) grep(x, W_levels))


ws_r <- as.numeric(df_learn$treatment_r) # respondent-level treatment only
ws_h <- as.numeric(df_learn$treatment_h) # headline-level treatment only
K_r <- length(unique(ws_r))
K_h <- length(unique(ws_h))
# Include headline as context
xs_h <- cbind(xs_learn, 
              model.matrix(lm(Y~ treatment_h-1, data = df_learn)))
# Predict when headline == control
xs_h_new <- xs_h
treat_cols <- grepl(pattern = 'treatment', colnames(xs_h))
xs_h_new[, treat_cols] <- matrix(c(1, rep(0, sum(treat_cols) -1 )), 
                                 nrow = nrow(df_learn), 
                                 ncol = sum(treat_cols), byrow = TRUE)

# Include respondent as context
xs_r <- cbind(xs_learn, 
              model.matrix(lm(Y~ treatment_r-1, data = df_learn)))
# Predict when headline == control
xs_r_new <- xs_r
treat_cols <- grepl(pattern = 'treatment', colnames(xs_r))
xs_r_new[, treat_cols] <- matrix(c(1, rep(0, sum(treat_cols) -1 )), 
                                 nrow = nrow(df_learn), 
                                 ncol = sum(treat_cols), byrow = TRUE)

```

### Learning scores
```{r probs_learning, cache = TRUE}
# For learning
probs_learn <- as.matrix(df_learn[, paste0('probs_', 0:39)])
balwts_learn <- (1/probs_learn)[cbind(1:A, ws_learn)]

# Re-calculate probs to aggregate across headline level
probs_rwide <- probs_learn
for(x in WH_idx){
  probs_rwide[,x] <- probs_learn[,x]/rowSums(probs_learn[,x])
}
balwts_r <- (1/probs_rwide)[matrix(c(1:nrow(df_learn),ws_learn), ncol = 2)]


probs_r <- t(sapply(1:nrow(df_learn), function(x){
  hval <- ws_h[x]
  colidx <- WH_idx[[hval]]
  probs_rwide[x, colidx]
} ))

# Re-calculate probs to aggregate across respondent level
probs_hwide <- probs_learn
for(x in WR_idx){
  probs_hwide[,x] <- probs_learn[,x]/rowSums(probs_learn[,x])
}
balwts_h <- (1/probs_hwide)[matrix(c(1:nrow(df_learn),ws_learn), ncol = 2)]


probs_h <- t(sapply(1:nrow(df_learn), function(x){
  rval <- ws_r[x]
  colidx <- WR_idx[[rval]]
  probs_hwide[x, colidx]
} ))
```

For scores in the learning split, we are interested in making comparisons across levels within the two types of factors: respondent-level treatments, and headline-level treatments. To share information across treatment conditions, for our conditional means model, we run separate multi-arm causal forests for each factor type. 

- For the respondent type factor, in the multi-arm causal forest model each *respondent* treatment is an arm, with the reference condition set as control. Headline treatments are treated as contexts. There are `r sum((df_learn$treatment_r=='control'))` observations in the control group, and a minimum and maximum of `r min(table(df_learn$treatment_r)[2:8])` and `r max(table(df_learn$treatment_r)[2:8])` in a given treatment group, respectively.
- For the headline type factor, in the multi-arm causal forest model each *headline* treatment is an arm, with the reference condition set as control. Respondent treatments are treated as contexts. There are `r sum((df_learn$treatment_h=='control'))` observations in the control group, and a minimum and maximum of `r min(table(df_learn$treatment_h)[2:5])` and `r max(table(df_learn$treatment_h)[2:5])` in a given treatment group, respectively.



```{r learn_scores, cache=TRUE}

# if(file.exists('objects/aipw_scores_learn.RDS')){ # read in scores if already generated
#   aipw_scores_learn <- readRDS('objects/aipw_scores_learn.RDS')
#   aipw_scoresR_learn <- aipw_scores_learn[[1]]
#   aipw_scoresRmarg_learn <- aipw_scores_learn[[2]]
#   aipw_scoresH_learn <- aipw_scores_learn[[3]]
#   aipw_scoresHmarg_learn <- aipw_scores_learn[[4]]
#   
# }else{
#   aipw_scores_learn <- aw_scores_learn(xs_h = xs_h, xs_r = xs_r, 
#                                        ws_h = ws_h, ws_r = ws_r, ws = ws_learn, 
#                                        yobs = yobs_learn, 
#                                        K_h = K_h, K_r = K_r, K = K, 
#                                        balwts = balwts_learn, 
#                                        balwts_r = balwts_r, balwts_h = balwts_h, 
#                                        probs_r = probs_r, probs_h = probs_h, 
#                                        probsK = probs_learn,
#                                        chunks = update_times)
#   
#   aipw_scoresR_learn <- aipw_scores_learn[[1]]
#   aipw_scoresRmarg_learn <- aipw_scores_learn[[2]]
#   aipw_scoresH_learn <- aipw_scores_learn[[3]]
#   aipw_scoresHmarg_learn <- aipw_scores_learn[[4]]
#   
#   saveRDS(aipw_scores_learn, 'objects/aipw_scores_learn.RDS')
#   
# }
```


For the evaluation scores, we consider only respondents who are assigned policies of interest:

- control
- headline: related articles
- headline: fact check
- respondent: accuracy nudge
- respondent: Facebook tips
- respondent: contextual policy, composed of accuracy nudge, Facebook tips, emotion nudge, and video treatment

Approximately 11% of respondents are assigned treatment uniformly at random; these respondents can be used for off-policy evaluation, but are not used in this analysis. 

To calculate scores, our conditional means model is a multi-arm causal forest, with the reference condition set as the pure control. We include five arms for each of the non-contextual policies; because the contextual policy is composed of treatments that overlap with the non-contextual policies, we do not include a contextual policy condition, but rather a condition for the contextual assignments that are not accounted for in the non-contextual arms, the emotion nudge and video treatment. 


### Evalution Scores
```{r eval_scores_setup, cache=TRUE}
# For evaluation
probs_eval <- as.matrix(df_eval[, paste0('probs_', 0:39)])
balwts_eval <- (1/probs_eval)[cbind(1:nrow(df_eval), df_eval$W)]
what_eval <- cbind(probs_eval[, c(1, 2, 5, 6, 11)], 
                   probs_eval[, 8] + probs_eval[, 12])

# Adjust probabilities for differences between learning and evaluation splits;
# target is eval pop re-weighted to learning distribution
ws_learn_eval <- rep(1, N)
ws_learn_eval[1:A] <- 0 # 0 for learning, 1 for evaluation split
learn_eval_forest <- multi_arm_causal_forest(X = rbind(xs_learn, xs_eval), 
                                             Y = c(yobs_learn, yobs_eval), 
                                             W = as.factor(ws_learn_eval))
probs_adjusted <- learn_eval_forest$W.hat

# probability of being in your own split over probability of being in learning split
# Weight is 1 for observations in the learning split
what_eval_adjustment <- (probs_adjusted[matrix(c(1:N, ws_learn_eval+1), ncol = 2)]/probs_adjusted[,1])[(A+1):N]
# Standardize to appropriate group size
what_eval_adjustment <- what_eval_adjustment*(N-A)/sum(what_eval_adjustment)

```

```{r optimal_policy_learning, cache=TRUE}
train_idx <- ws_r %in%c(2,6)

cf.priority <- causal_forest(
  X = xs_learn[train_idx, ],
  Y = df_learn$post_false_prop[train_idx],
  W = 1*(ws_r[train_idx]==2),
  W.hat = probs_r[train_idx, 2]/(rowSums(probs_r[train_idx, c(2,7)])),
  seed = 60637)

optimal_assignment <- ifelse(predict(cf.priority, xs_eval)$predictions<0, 4, 5)

# colMeans(aipw_scores_any_false_eval)
# prop.table(table(optimal_assignment))
# mean(aipw_scores_any_false_eval[cbind(1:nrow(df_eval), optimal_assignment)])

df_eval$optimal_assignment <- factor(optimal_assignment,
                                     labels = c('Optimal policy == accuracy',
                                                'Optimal policy == FB tips'))
```

```{r combined_eval_scores, cache=TRUE}
## Combined response function scores 
if(file.exists('objects/aipw_scores_eval.RDS')){ # read in scores if already generated
  aipw_scores <- readRDS('objects/aipw_scores_eval.RDS')
  
}else{
  aipw_scores <- aw_scores_eval(xs = xs_eval,
                                yobs = yobs_eval, 
                                ws = ws_eval, 
                                # what = what_eval, 
                                optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores, 'objects/aipw_scores_eval.RDS')
  
}


if(file.exists('objects/aipw_scores_eval_adjusted.RDS')){ # read in scores if already generated
  aipw_scores_adjusted <- readRDS('objects/aipw_scores_eval_adjusted.RDS')
  
}else{
  aipw_scores_adjusted <- aw_scores_eval(xs = xs_eval,
                                         yobs = yobs_eval, 
                                         ws = ws_eval, 
                                         sample.weights = what_eval_adjustment, 
                                         optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores_adjusted, 'objects/aipw_scores_eval_adjusted.RDS')
}

colnames(aipw_scores) <- colnames(aipw_scores_adjusted) <- treatment_levels

```

```{r any_eval_scores, cache = TRUE}
## Any share scores
any_true_eval <- df_eval$post_true_prop
any_false_eval<- df_eval$post_false_prop

if(file.exists('objects/aipw_scores_any_true_eval.RDS')){ # read in scores if already generated
  aipw_scores_any_true_eval <- readRDS('objects/aipw_scores_any_true_eval.RDS')
  
}else{
  aipw_scores_any_true_eval <- aw_scores_eval(xs = xs_eval,
                                              yobs = any_true_eval, 
                                              ws = ws_eval, 
                                              what = what_eval, 
                                              optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores_any_true_eval, 'objects/aipw_scores_any_true_eval.RDS')
}


if(file.exists('objects/aipw_scores_any_false_eval.RDS')){ # read in scores if already generated
  aipw_scores_any_false_eval <- readRDS('objects/aipw_scores_any_false_eval.RDS')
  
}else{
  aipw_scores_any_false_eval <- aw_scores_eval(xs = xs_eval,
                                               yobs = any_false_eval, 
                                               ws = ws_eval, 
                                               # what = what_eval, 
                                               optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores_any_false_eval, 'objects/aipw_scores_any_false_eval.RDS')
}

if(file.exists('objects/aipw_scores_any_true_eval_adjusted.RDS')){ # read in scores if already generated
  aipw_scores_any_true_eval_adjusted <- readRDS('objects/aipw_scores_any_true_eval_adjusted.RDS')
  
}else{
  aipw_scores_any_true_eval_adjusted <- aw_scores_eval(xs = xs_eval,
                                                       yobs = any_true_eval, 
                                                       ws = ws_eval, 
                                                       sample.weights = what_eval_adjustment, 
                                                       optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores_any_true_eval_adjusted, 'objects/aipw_scores_any_true_eval_adjusted.RDS')
}


if(file.exists('objects/aipw_scores_any_false_eval_adjusted.RDS')){ # read in scores if already generated
  aipw_scores_any_false_eval_adjusted <- readRDS('objects/aipw_scores_any_false_eval_adjusted.RDS')
  
}else{
  aipw_scores_any_false_eval_adjusted <- aw_scores_eval(xs = xs_eval,
                                                        yobs = any_false_eval, 
                                                        ws = ws_eval, 
                                                        sample.weights = what_eval_adjustment,
                                                        optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores_any_false_eval_adjusted, 'objects/aipw_scores_any_false_eval_adjusted.RDS')
}
```

```{r channel_eval_scores, cache = TRUE, message=FALSE, warning=FALSE}
## Channel scores
timeline_true_eval <- df_eval$post_true_timeline_prop
timeline_false_eval <- df_eval$post_false_timeline_prop
send_true_eval <- df_eval$post_true_send_prop
send_false_eval <- df_eval$post_false_send_prop

if(file.exists('objects/aipw_scores_timeline_true_eval.RDS')){ # read in scores if already generated
  aipw_scores_timeline_true_eval <- readRDS('objects/aipw_scores_timeline_true_eval.RDS')
  
}else{
  
  aipw_scores_timeline_true_eval <- aw_scores_eval(xs = xs_eval,
                                                   yobs = timeline_true_eval, 
                                                   ws = ws_eval, 
                                                   #                                                   what = what_eval, 
                                                   optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores_timeline_true_eval, 'objects/aipw_scores_timeline_true_eval.RDS')
  
}

if(file.exists('objects/aipw_scores_timeline_false_eval.RDS')){ # read in scores if already generated
  aipw_scores_timeline_false_eval <- readRDS('objects/aipw_scores_timeline_false_eval.RDS')
  
}else{
  
  aipw_scores_timeline_false_eval <- aw_scores_eval(xs = xs_eval,
                                                    yobs = timeline_false_eval, 
                                                    ws = ws_eval, 
                                                    #                                                    what = what_eval, 
                                                    optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores_timeline_false_eval, 'objects/aipw_scores_timeline_false_eval.RDS')
  
}


if(file.exists('objects/aipw_scores_send_true_eval.RDS')){ # read in scores if already generated
  aipw_scores_send_true_eval <- readRDS('objects/aipw_scores_send_true_eval.RDS')
  
}else{
  
  aipw_scores_send_true_eval <- aw_scores_eval(xs = xs_eval,
                                               yobs = send_true_eval, 
                                               ws = ws_eval, 
                                               #                                               what = what_eval, 
                                               optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores_send_true_eval, 'objects/aipw_scores_send_true_eval.RDS')
}

if(file.exists('objects/aipw_scores_send_false_eval.RDS')){ # read in scores if already generated
  aipw_scores_send_false_eval <- readRDS('objects/aipw_scores_send_false_eval.RDS')
  
}else{
  aipw_scores_send_false_eval <- aw_scores_eval(xs = xs_eval,
                                                yobs = send_false_eval, 
                                                ws = ws_eval, 
                                                #                                                what = what_eval, 
                                                optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores_send_false_eval, 'objects/aipw_scores_send_false_eval.RDS')
}


# adjusted


if(file.exists('objects/aipw_scores_timeline_true_eval_adjusted.RDS')){ # read in scores if already generated
  aipw_scores_timeline_true_eval_adjusted <- readRDS('objects/aipw_scores_timeline_true_eval_adjusted.RDS')
  
}else{
  
  aipw_scores_timeline_true_eval_adjusted <- aw_scores_eval(xs = xs_eval,
                                                            yobs = timeline_true_eval, 
                                                            ws = ws_eval, 
                                                            sample.weights =  what_eval_adjustment, 
                                                            optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores_timeline_true_eval_adjusted, 'objects/aipw_scores_timeline_true_eval_adjusted.RDS')
  
}

if(file.exists('objects/aipw_scores_timeline_false_eval_adjusted.RDS')){ # read in scores if already generated
  aipw_scores_timeline_false_eval_adjusted <- readRDS('objects/aipw_scores_timeline_false_eval_adjusted.RDS')
  
}else{
  
  aipw_scores_timeline_false_eval_adjusted <- aw_scores_eval(xs = xs_eval,
                                                             yobs = timeline_false_eval, 
                                                             ws = ws_eval, 
                                                             sample.weights =  what_eval_adjustment, 
                                                             optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores_timeline_false_eval_adjusted, 'objects/aipw_scores_timeline_false_eval_adjusted.RDS')
  
}


if(file.exists('objects/aipw_scores_send_true_eval_adjusted.RDS')){ # read in scores if already generated
  aipw_scores_send_true_eval_adjusted <- readRDS('objects/aipw_scores_send_true_eval_adjusted.RDS')
  
}else{
  
  aipw_scores_send_true_eval_adjusted <- aw_scores_eval(xs = xs_eval,
                                                        yobs = send_true_eval, 
                                                        ws = ws_eval, 
                                                        sample.weights = what_eval_adjustment, 
                                                        optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores_send_true_eval_adjusted, 'objects/aipw_scores_send_true_eval_adjusted.RDS')
}

if(file.exists('objects/aipw_scores_send_false_eval_adjusted.RDS')){ # read in scores if already generated
  aipw_scores_send_false_eval_adjusted <- readRDS('objects/aipw_scores_send_false_eval_adjusted.RDS')
  
}else{
  aipw_scores_send_false_eval_adjusted <- aw_scores_eval(xs = xs_eval,
                                                         yobs = send_false_eval, 
                                                         ws = ws_eval, 
                                                         sample.weights =  what_eval_adjustment, 
                                                         optimal_assignment = optimal_assignment)
  
  saveRDS(aipw_scores_send_false_eval_adjusted, 'objects/aipw_scores_send_false_eval_adjusted.RDS')
}
```

# Main Analysis

* Notes on analysis:
+ Outcomes:
1. ) Sharing intentions as formalized in primary response function (Y =−Mb+0.5Tb, pre-analysis plan p. 17)
2. ) Sharing intentions for true/false stimuli separately (as proportion of stimuli of each type seen)
3. ) Sharing intentions for true/false stimuli and by timeline/messenger channel separately (as proportion of stimuli of each type seen)
4. ) Rates of clicking on *true* stories, combined pre- and post-treatment, i.e., the percent of true stimuli that the respondent said they wanted to share during the survey for which they later click the button to share on Facebook (described in pre-analysis plan section 3.4.2)
5. ) Rates of clicking on debriefs about *false* stories, combined pre- and post-treatment, i.e., the percent of false stimuli that the respondent was debriefed on for which they later click the button to learn more about on (described in pre-analysis plan section 3.4.2)
+ Use the last batch of data only
+ Reported results are also weighted by the inverse probability of appearing in the last batch, as predicted by a regression forest 
+ For respondents who attrit after collection of pre-test responses and before collection of post-test responses, the post-test interest in sharing response function will be coded as identical to the individual pre-test value; for behavioral sharing outcomes, we impute zeros for click-through-rates.
* Hypotheses
+	Uniform policy:
- The best uniform headline-level policy improves response over the control policy.
- The best uniform respondent-level policy improves response over the control policy.
+ Contextual policy (primary): 
-  The best contextual policy achieves higher value than the control treatment
-  The best contextual policy achieves higher value than the best uniform headline-level treatment
-  The best contextual policy achieves higher value than the best uniform respondent-level treatment

```{r eval_setup, cache = TRUE}
covariate_names <- c(context_cols, 'Y_pre')
lin_covs <- formula(paste0(' ~ ', 
                           paste(covariate_names, collapse = ' + ')))
iter <- 1e3 # iterations for bootstraps
```


```{r combined_response_eval_estimation, cache=TRUE}
# Scores estimates

# mean response
aipw_eval_est <- find_est(aipw_scores)

# treatment effects
aipw_eval_te <- find_te(aipw_scores)

## Covariate-shift adjusted
aipw_eval_adj_est <- find_adj_est(aipw_scores_adjusted, what_eval_adjustment)
aipw_eval_adj_te <- find_adj_te(aipw_scores_adjusted, what_eval_adjustment)

```

```{r any_response_eval_estimation, cache=TRUE}
# Scores estimates
## true
aipw_eval_t_est <- find_est(aipw_scores_any_true_eval)
aipw_eval_t_te <- find_te(aipw_scores_any_true_eval)

## false
aipw_eval_f_est <- find_est(aipw_scores_any_false_eval)
aipw_eval_f_te <- find_te(aipw_scores_any_false_eval)

## Covariate-shift adjusted
## true
aipw_eval_t_adj_est <- find_adj_est(aipw_scores_any_true_eval_adjusted, what_eval_adjustment)
aipw_eval_t_adj_te <- find_adj_te(aipw_scores_any_true_eval_adjusted, what_eval_adjustment)

## false
aipw_eval_f_adj_est <- find_adj_est(aipw_scores_any_false_eval_adjusted, what_eval_adjustment)
aipw_eval_f_adj_te <- find_adj_te(aipw_scores_any_false_eval_adjusted, what_eval_adjustment)
```

```{r channel_response_eval_estimation, cache=TRUE}
# Scores estimates
## true
aipw_eval_tt_est <- find_est(aipw_scores_timeline_true_eval)
aipw_eval_st_est <- find_est(aipw_scores_send_true_eval)

aipw_eval_tt_te <- find_te(aipw_scores_timeline_true_eval)
aipw_eval_st_te <- find_te(aipw_scores_send_true_eval)

## false
aipw_eval_tf_est <- find_est(aipw_scores_timeline_false_eval)
aipw_eval_sf_est <- find_est(aipw_scores_send_false_eval)

aipw_eval_tf_te <- find_te(aipw_scores_timeline_false_eval)
aipw_eval_sf_te <- find_te(aipw_scores_send_false_eval)

## Covariate-shift adjusted
## true
aipw_eval_tt_adj_est <- find_adj_est(aipw_scores_timeline_true_eval_adjusted, what_eval_adjustment)
aipw_eval_st_adj_est <- find_adj_est(aipw_scores_send_true_eval_adjusted, what_eval_adjustment)

aipw_eval_tt_adj_te <- find_adj_te(aipw_scores_timeline_true_eval_adjusted, what_eval_adjustment)
aipw_eval_st_adj_te <- find_adj_te(aipw_scores_send_true_eval_adjusted, what_eval_adjustment)

## false
aipw_eval_tf_adj_est <- find_adj_est(aipw_scores_timeline_false_eval_adjusted, what_eval_adjustment)
aipw_eval_sf_adj_est <- find_adj_est(aipw_scores_send_false_eval_adjusted, what_eval_adjustment)

aipw_eval_tf_adj_te <- find_adj_te(aipw_scores_timeline_false_eval_adjusted, what_eval_adjustment)
aipw_eval_sf_adj_te <- find_adj_te(aipw_scores_send_false_eval_adjusted, what_eval_adjustment)
```

```{r label_scores, cache=TRUE}

aipw_eval_est$term <- aipw_eval_adj_est$term <- 
  aipw_eval_t_est$term <- aipw_eval_t_adj_est$term <- 
  aipw_eval_f_est$term <- aipw_eval_f_adj_est$term <- 
  aipw_eval_tt_est$term <- aipw_eval_tf_est$term <- 
  aipw_eval_st_est$term <- aipw_eval_sf_est$term <-
  aipw_eval_tt_adj_est$term <- aipw_eval_tf_adj_est$term <- 
  aipw_eval_st_adj_est$term <- aipw_eval_sf_adj_est$term <- 
  treatment_levels

aipw_eval_te$term <- aipw_eval_adj_te$term <- 
  aipw_eval_f_te$term <- aipw_eval_f_adj_te$term <- 
  aipw_eval_t_te$term <- aipw_eval_t_adj_te$term <- 
  aipw_eval_t_te$term <- aipw_eval_t_adj_te$term <- 
  aipw_eval_tt_te$term <- aipw_eval_tf_te$term <- 
  aipw_eval_st_te$term <- aipw_eval_sf_te$term <-
  aipw_eval_tt_adj_te$term <- aipw_eval_tf_adj_te$term <- 
  aipw_eval_st_adj_te$term <- aipw_eval_sf_adj_te$term <-
  treatment_levels[-1]

```


## Mean response by type of stimuli


```{r any_eval_scoresplot, echo = FALSE, message = FALSE, cache=TRUE, warning=FALSE}

any_scores <- bind_rows(list(`True` = aipw_eval_t_est,
                             `False` = aipw_eval_f_est), 
                        .id = 'Stimuli Type') %>% 
  mutate(`Stimuli Type` = relevel(as.factor(`Stimuli Type`),
                                  ref = 'True'))

any_scores$Intervention <- factor(rep(rep(c('Control', 'Headline', 'Respondent'), 
                                          times = c(1, 2,3)), 2))

ggplot(any_scores, aes(x = estimate, y = term, color=`Stimuli Type`)) +
  stat_gradientinterval(aes(y = term, 
                            xdist = distributional::dist_normal(estimate, std.error), 
                            color=`Stimuli Type`, fill = `Stimuli Type`), 
                        position = position_dodge(width=0.5), .width = 0, size = 0) +
  facet_grid(rows = vars(Intervention), scales = 'free_y', space = 'free_y') + 
  geom_errorbar(aes(xmin = estimate - 1.96*std.error, xmax = estimate + 1.96*std.error), width = 0.05, position=position_dodge(width=0.5)) +
  scale_color_manual(values = cbPalette[c(6,7)]) +
  scale_fill_manual(values = cbPalette[c(6,7)]) +
  geom_point(aes(x = estimate), size = 2, position=position_dodge(width=0.5)) +
  ylab('Policy') + 
  xlab('Estimate') +
  # geom_vline(xintercept = 0, colour = 'grey60', linetype = 2) + 
  ggtitle(label = 'Average response', 
          subtitle = 'Evaluation data') +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5),
        panel.background = element_rect(fill = 'white'),
        plot.background = element_rect(fill = 'white', color = 'white'),
        strip.text.y = element_blank()) +
  coord_cartesian(xlim = c(0.35, 0.7))

ggsave('../figures/evaluated_scores_any.png', width = 8, height = 6)
```


## Mean response by type of stimuli and channel

```{r channel_eval_scoresplot, echo = FALSE, message = FALSE, cache=TRUE, warning=FALSE, message=FALSE}
channel_scores <- bind_rows(list(
  `Timeline` = bind_rows(list(`True` = aipw_eval_tt_est,
                              `False` = aipw_eval_tf_est), 
                         .id = 'Stimuli Type'),
  `Messenger` = bind_rows(list(`True` = aipw_eval_st_est,
                               `False` = aipw_eval_sf_est), 
                          .id = 'Stimuli Type')), 
  .id = 'Channel') %>% 
  mutate(`Stimuli Type` = relevel(as.factor(`Stimuli Type`),
                                  ref = 'True'),
         `Channel` = relevel(as.factor(`Channel`),
                             ref = 'Timeline'))

channel_scores$Intervention <- factor(rep(c('Control', rep(c('Headline', 'Respondent'), times = c(2,3))),4))
ggplot(channel_scores, aes(x = estimate, y =term, color=`Stimuli Type`, 
                           shape = Channel)) +
  stat_gradientinterval(aes(y = term, 
                            xdist = distributional::dist_normal(estimate, std.error), 
                            color=`Stimuli Type`, fill = `Stimuli Type`), 
                        position = position_dodge(width=0.5), .width = 0, size = 0) +
  geom_point(position=position_dodge(width=0.5), 
             size = 4, fill = 'white') + # change size, fill here
  geom_errorbar(aes(xmin = estimate - 1.96*std.error, xmax = estimate + 1.96*std.error), 
                width = 0.05, position=position_dodge(width=0.5)) + 
  scale_shape_manual(values = c(21, 24)) + # change shape here
  facet_grid(rows = vars(Intervention), scales = 'free_y', space = 'free_y') + 
  xlab('Treatment') +
  ylab('Estimate') +
  coord_cartesian(xlim=c(.3, .65)) +
  geom_vline(xintercept = 0, colour = 'grey60', linetype = 2) +
  scale_color_manual(values = cbPalette[c(6,7)]) +
  scale_fill_manual(values = cbPalette[c(6,7)]) +
  ggtitle(label = 'Average response', 
          subtitle = 'Evaluation data') +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5),
        panel.background = element_rect(fill = 'white'),
        plot.background = element_rect(fill = 'white', color = 'white'),
        strip.text.y = element_blank())

ggsave('../figures/evaluated_scores_channel.png', width = 8, height = 6)
```

## Mean response across response measures
```{r unified_eval_scoresplot, echo = FALSE, message = FALSE, cache=TRUE, warning=FALSE, message=FALSE}

all_scores <- bind_rows(list(
  `Any` = bind_rows(list(#`True` = aipw_eval_t_est,
    #`False` = aipw_eval_f_est,
    `Combined` = aipw_eval_est), 
    .id = 'Response measure'), 
  `Timeline` = bind_rows(list(`True` = aipw_eval_tt_est,
                              `False` = aipw_eval_tf_est), 
                         .id = 'Response measure'),
  `Messenger` = bind_rows(list(`True` = aipw_eval_st_est,
                               `False` = aipw_eval_sf_est), 
                          .id = 'Response measure')), 
  .id = 'Channel') %>% 
  mutate(`Response measure` = factor(`Response measure`,
                                     levels = c('False','True', 'Combined')),
         `Channel` = relevel(as.factor(`Channel`),
                             ref = 'Timeline'),
         `Aggregation` = factor(case_when(
           `Response measure` == 'Combined' ~ 'Combined',
           TRUE ~ 'Disaggregated'),
           levels = c('Disaggregated', 'Combined')))

all_scores$Intervention <- factor(rep(c('Control', rep(c('Headline', 'Respondent'), times = c(2,3))),5))

ggplot(all_scores, aes(x = estimate, y =term, color=`Response measure`, 
                       shape = Channel)) +
  stat_gradientinterval(
    data = all_scores %>% filter(Channel != 'Any'),
    aes(y = term, 
        xdist = distributional::dist_normal(estimate, std.error), 
        color=`Response measure`, fill = `Response measure`), 
    position = position_dodge(width=0.5), .width = 0, size = 0) +
  stat_gradientinterval(
    data = all_scores %>% filter(Channel == 'Any'),
    aes(y = term, 
        xdist = distributional::dist_normal(estimate, std.error), 
        color=`Response measure`, fill = `Response measure`), 
    .width = 0, size = 0, height = 0.2)  +
  geom_point(data = all_scores %>% filter(Channel != 'Any'),
             position=position_dodge(width=0.5), 
             size = 4, fill = 'white') + # change size, fill here
  geom_point(data = all_scores %>% filter(Channel == 'Any'), 
             size = 4, fill = 'white') + # change size, fill here
  geom_errorbar(data = all_scores %>% filter(Channel != 'Any'),
                aes(xmin = estimate - 1.96*std.error, xmax = estimate + 1.96*std.error), 
                width = 0.05, position=position_dodge(width=0.5)) + 
  geom_errorbar(data = all_scores %>% filter(Channel == 'Any'),
                aes(xmin = estimate - 1.96*std.error, xmax = estimate + 1.96*std.error), 
                width = 0) + 
  scale_shape_manual(values = c(21, 22, 24)) + # change shape here
  facet_grid(rows = vars(Intervention), 
             cols = vars(Aggregation),
             scales = 'free', space = 'free') + 
  xlab('Estimate') +
  ylab('Policy') +
  scale_color_manual(values = cbPalette[c(2,7,6)]) +
  scale_fill_manual(values = cbPalette[c(2,7,6)]) +
  ggtitle(label = 'Average response', 
          subtitle = 'Evaluation data') +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5),
        panel.background = element_rect(fill = 'white'),
        plot.background = element_rect(fill = 'white', color = 'white'),
        strip.text.y = element_blank(),
        strip.text.x = element_blank())

ggsave('../figures/evaluated_scores_all.png', width = 8, height = 4)
```

## Tables
```{r response_eval_tables, warning=FALSE}
# Mean response estimates
models <- list(
  `Combined` = list(
    tidy = bind_rows(aipw_eval_te,  aipw_eval_est[1,]),
    pval = 'p.value.upper'),
  `False: Any sharing` = list(
    tidy = bind_rows(aipw_eval_f_te,  aipw_eval_f_est[1,]),
    pval = 'p.value.lower'),
  `False: Messenger` = list(
    tidy = bind_rows(aipw_eval_sf_te,  aipw_eval_sf_est[1,]),
    pval = 'p.value.lower'),
  `False: Timeline` = list(
    tidy = bind_rows(aipw_eval_tf_te,  aipw_eval_tf_est[1,]),
    pval = 'p.value.lower'),
  `True: Any sharing` = list(
    tidy = bind_rows(aipw_eval_t_te,  aipw_eval_t_est[1,]),
    pval = 'p.value.upper'),
  `True: Messenger` = list(
    tidy = bind_rows(aipw_eval_st_te,  aipw_eval_st_est[1,]),
    pval = 'p.value.upper'),
  `True: Timeline` = list(
    tidy = bind_rows(aipw_eval_tt_te,  aipw_eval_tt_est[1,]),
    pval = 'p.value.upper'
    )
)


models <- lapply(models, function(x){
  class(x) <- "modelsummary_list"
  pvals <- x$tidy[,x$pval]
  pvals[length(pvals)] <- NA
  x$tidy$p.value <- pvals
  x
})

modelsummary(models, stars = TRUE, title = 'Mean response estimates, evaluation')
modelsummary(models, stars = TRUE,  title = 'Mean response estimates, evaluation', 
             output = '../tables/evaluated_estimates.tex')


#############
# the new modelsummary
rows <- tribble(~term, ~Combined, ~a, ~b, ~c, ~d, ~e, ~f,
                '', '', '', r'({\textbf{False}})', '', '', r'({\textbf{True}})', '',
                '', r'(\textbf{Combined})', 'Any sharing', 'Messenger', 'Timeline', 'Any sharing', 'Messenger', 'Timeline',
                r'(\cmidrule(lr){2-2} \cmidrule(lr){3-5} \cmidrule(lr){6-8})', '', '', '', '', '', '', '',
                r'(\multicolumn{2}{l}{\textbf{Headline treatment effects}} \rule{0pt}{1.2\normalbaselineskip})', '', '', '', '', '', '', '',
                r'(\multicolumn{2}{l}{\textbf{Respondent treatment effects}} \rule{0pt}{1.2\normalbaselineskip})', '', '', '', '', '', '', '',
                r'(\hline)', '', '', '', '', '', '', ''
               )

attr(rows, 'position') <- c(1,2,3,
                            4,9,
                            16)

modelsummary(models, stars = TRUE, title = 'Mean response estimates, evaluation', add_rows = rows, escape = FALSE) 

modelsummary(models, 
             stars = TRUE,  
             title = 'Mean response estimates, evaluation', 
             add_rows = rows,
             escape = FALSE,
             coef_rename = c('Headline\nFactcheck' = r'(\hspace{1em}Factcheck)',
                             'Headline\nRelated articles' = r'(\hspace{1em}Related articles)',
                             'Respondent\nAccuracy' = r'(\hspace{1em}Accuracy)',
                             'Respondent\nFacebook tips' = r'(\hspace{1em}Facebook tips)',
                             'Respondent\nOptimal' = r'(\hspace{1em}Optimal)',
                             'Control' = r'(\hspace{1em}Control mean)'), 
             output = '../tables/evaluated_estimates.tex')

# print out n's
models$Combined$tidy$term
```

### Control heterogeneity tables

```{r control_heterogeneity_primary, cache = TRUE}
covariate_list <- c('age', 'male', 'pol', 'dli', 'science')
stimuli_types <- c('true', 'false')
channel_types <- c('send', 'timeline')

table <- sapply(covariate_list, function(x) {
  covariate <- x
  
  if(length(unique(df_eval[, covariate])) <= 2){
    # binary covariates
    fmla0 <- formula(paste0('aipw_scores[,1]  ~  factor(',covariate, ') -1'))
    fmla1 <- formula(paste0('aipw_scores[,1] ~  factor(',covariate, ')'))
    ns <- table(df_eval[,covariate])
  }else{
    # continuous covariates
    adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
    fmla0 <- formula(paste0('aipw_scores[,1] ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla1 <- formula(paste0('aipw_scores[,1] ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    ns <- table(df_eval[,covariate] > median(df_eval[,covariate]) + adjustment)
  }
  ols0 <- tidy(lm_robust(fmla0, data=df_eval))
  ols1 <- tidy(lm_robust(fmla1, data=df_eval))
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate, ols1$estimate[2])
  est_std.err <- c(ols0$std.error, ols1$std.error[2])
  p_value <- c(NA, NA, ols1$p.value[2])
  ns <- c(ns, sum(ns))
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = ns,
                      `Covariate type` = str_to_sentence(covariate),
                      Type = c(0,1,2),
                      check.names = FALSE)
  
  outdf
  
},
simplify = FALSE)
table <- do.call(rbind, table)

table_combined <- table %>% 
  mutate(
    label = case_when(
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 0 ~ 
        paste0('Below median (n = ', n, ')'),
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 1 ~ 
        paste0('Above median (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 0 ~ 
        paste0(' Not male (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 1 ~ 
        paste0('Male (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 0 ~ 
        paste0('Kenya (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 1 ~ 
        paste0('Nigeria (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 0 ~ 
        paste0(' Not aligned (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 1 ~ 
        paste0('Aligned w/ ruling party (n = ', n, ')'),
      TRUE ~ 'Difference'),
    `Covariate type` = case_when(`Covariate type` == 'Dli' ~ 
                                   'Digital literacy index',
                                 `Covariate type` == 'Hhi' ~ 
                                   'Household Wealth Index',
                                 `Covariate type` == 'Nigeria' ~ 
                                   'Country',
                                 `Covariate type` == 'Pol' ~ 
                                   'Political Allegiance',
                                 `Covariate type` == 'Science' ~ 
                                   'Scientific beliefs index',
                                 TRUE ~ `Covariate type`),
    term = paste0(`Covariate type`, ': ', label)
  )

```

```{r control_heterogeneity_any, cache = TRUE}

apply_mat <- expand.grid(stimuli_types, covariate_list)
table <- apply(apply_mat, 1, function(x) {
  stimuli_type <- x[1]
  covariate <- x[2]
  
  if(length(unique(df_eval[, covariate])) <= 2){
    # binary covariates
    fmla0 <- formula(paste0('aipw_scores_any_',stimuli_type,
                            '_eval[,1] ~  factor(',covariate, ') -1'))
    fmla1 <- formula(paste0('aipw_scores_any_',stimuli_type,
                            '_eval[,1] ~  factor(',covariate, ')'))
    ns <- table(df_eval[,covariate])
  }else{
    # continuous covariates
    adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
    fmla0 <- formula(paste0('aipw_scores_any_',stimuli_type, 
                            '_eval[,1] ~  factor(',covariate, 
                            '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla1 <- formula(paste0('aipw_scores_any_',stimuli_type, 
                            '_eval[,1] ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    ns <- table(df_eval[,covariate] > median(df_eval[,covariate]) + adjustment)
  }
  ols0 <- tidy(lm_robust(fmla0, data=df_eval))
  ols1 <- tidy(lm_robust(fmla1, data=df_eval))
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate, ols1$estimate[2])
  est_std.err <- c(ols0$std.error, ols1$std.error[2])
  p_value <- c(NA, NA, ols1$p.value[2])
  ns <- c(ns, sum(ns))
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = ns,
                      `Stimuli type` = str_to_sentence(stimuli_type),
                      `Covariate type` = str_to_sentence(covariate),
                      Type = c(0,1,2),
                      check.names = FALSE)
  
  outdf
  
})
table <- do.call(rbind, table)

table_any <- table %>% 
  mutate(
    `Stimuli type` = factor(`Stimuli type`, 
                            levels = c('True', 'False')), 
    label = case_when(
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 0 ~ 
        paste0('Below median (n = ', n, ')'),
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 1 ~ 
        paste0('Above median (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 0 ~ 
        paste0(' Not male (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 1 ~ 
        paste0('Male (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 0 ~ 
        paste0('Kenya (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 1 ~ 
        paste0('Nigeria (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 0 ~ 
        paste0(' Not aligned (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 1 ~ 
        paste0('Aligned w/ ruling party (n = ', n, ')'),
      TRUE ~ 'Difference'),
    `Covariate type` = case_when(`Covariate type` == 'Dli' ~ 
                                   'Digital literacy index',
                                 `Covariate type` == 'Hhi' ~ 
                                   'Household Wealth Index',
                                 `Covariate type` == 'Nigeria' ~ 
                                   'Country',
                                 `Covariate type` == 'Pol' ~ 
                                   'Political Allegiance',
                                 `Covariate type` == 'Science' ~ 
                                   'Scientific beliefs index',
                                 TRUE ~ `Covariate type`),
    term = paste0(`Covariate type`, ': ', label)
  )

```

```{r control_heterogeneity_channel, cache = TRUE}
apply_mat <- expand.grid(stimuli_types, channel_types, covariate_list)
table <- apply(apply_mat, 1, function(x) {
  stimuli_type <- x[1]
  covariate <- x[3]
  channel <- x[2]
  
  if(length(unique(df_eval[, covariate])) <= 2){
    # binary covariates
    fmla0 <- formula(paste0('aipw_scores_', channel, '_', stimuli_type, '_eval[,1]', 
                            
                            ' ~  factor(',covariate, ') -1'))
    fmla1 <- formula(paste0('aipw_scores_', channel, '_', stimuli_type, '_eval[,1]', 
                            ' ~  factor(',covariate, ')'))
    ns <- table(df_eval[,covariate])
  }else{
    # continuous covariates
    adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
    fmla0 <- formula(paste0('aipw_scores_', channel, '_',stimuli_type, '_eval[,1]',
                            ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla1 <- formula(paste0('aipw_scores_', channel, '_',stimuli_type, '_eval[,1]',
                            ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    ns <- table(df_eval[,covariate] > median(df_eval[,covariate]) + adjustment)
  }
  ols0 <- tidy(lm_robust(fmla0, data=df_eval))
  ols1 <- tidy(lm_robust(fmla1, data=df_eval))
  
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate, ols1$estimate[2])
  est_std.err <- c(ols0$std.error, ols1$std.error[2])
  p_value <- c(NA, NA, ols1$p.value[2])
  ns <- c(ns, sum(ns))
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = ns,
                      `Stimuli type` = str_to_sentence(stimuli_type),
                      `Covariate type` = str_to_sentence(covariate),
                      `Channel` = channel,
                      Type = c(0,1,2),
                      check.names = FALSE)
  
  outdf
  
})


table <- do.call(rbind, table)

table_channel <- table %>% 
  mutate(
    `Stimuli type` = factor(`Stimuli type`, 
                            levels = c('True', 'False')), 
    label = case_when(
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 0 ~ 
        paste0('Below median (n = ', n, ')'),
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 1 ~ 
        paste0('Above median (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 0 ~ 
        paste0(' Not male (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 1 ~ 
        paste0('Male (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 0 ~ 
        paste0('Kenya (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 1 ~ 
        paste0('Nigeria (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 0 ~ 
        paste0(' Not aligned (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 1 ~ 
        paste0('Aligned w/ ruling party (n = ', n, ')'),
      TRUE ~ 'Difference'),
    `Covariate type` = case_when(`Covariate type` == 'Dli' ~ 
                                   'Digital literacy index',
                                 `Covariate type` == 'Hhi' ~ 
                                   'Household Wealth Index',
                                 `Covariate type` == 'Nigeria' ~ 
                                   'Country',
                                 `Covariate type` == 'Pol' ~ 
                                   'Political Allegiance',
                                 `Covariate type` == 'Science' ~ 
                                   'Scientific beliefs index',
                                 TRUE ~ `Covariate type`),
    term = paste0(`Covariate type`, ': ', label)
  )



```

```{r control_combined_heterogeneity, cache = TRUE}
# Mean response estimates
models <- list(
  Combined = list(tidy = table_combined),
  `False: Any sharing` = list(tidy = table_any[which(table_any$`Stimuli type` == 'False'),]),
  `False: Messenger` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'False' &
                                                         table_channel$Channel == 'send'),]),
  `False: Timeline` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'False' &
                                                        table_channel$Channel == 'timeline'),]),
  `True: Any sharing` = list(tidy = table_any[which(table_any$`Stimuli type` == 'True'),]),
  `True: Messenger` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'True' &
                                                        table_channel$Channel == 'send'),]),
  `True: Timeline` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'True' &
                                                       table_channel$Channel == 'timeline'),])
)

models <- lapply(models, function(x){
  class(x) <- "modelsummary_list"
  x
})

#############
# the original modelsummary
modelsummary(models, stars = TRUE, title = 'Heterogeneity under control, evaluation') 

modelsummary(models, stars = TRUE,  title = 'Heterogeneity under control, evaluation', 
             output = '../tables/heterogeneity_control.tex')


#############
# the new modelsummary
rows <- tribble(~term, ~Combined, ~a, ~b, ~c, ~d, ~e, ~f,
                '', '', '', r'({\textbf{False}})', '', '', r'({\textbf{True}})', '',
                '', r'(\textbf{Combined})', 'Any sharing', 'Messenger', 'Timeline', 'Any sharing', 'Messenger', 'Timeline',
                 r'(\cmidrule(lr){2-2} \cmidrule(lr){3-5} \cmidrule(lr){6-8} \multicolumn{4}{l}{\textbf{Overall control mean}} \rule{0pt}{1.2\normalbaselineskip})', '', '', '', '', '', '', '',

                r'(\hline \multicolumn{2}{l}{\textbf{Age}} \rule{0pt}{1.2\normalbaselineskip})', '', '', '', '', '', '', '',
                r"(\cmidrule(lr){2-8})", '', '', '', '', '', '', '',
                r'(\multicolumn{2}{l}{\textbf{Gender}} \rule{0pt}{1.2\normalbaselineskip})', '', '', '', '', '', '', '',
                r"(\cmidrule(lr){2-8})", '', '', '', '', '', '', '',
                r'(\multicolumn{2}{l}{\textbf{Political allegiance}} \rule{0pt}{1.2\normalbaselineskip})', '', '', '', '', '', '', '',
                r"(\cmidrule(lr){2-8})", '', '', '', '', '', '', '',
                r'(\multicolumn{2}{l}{\textbf{Digital literacy index}} \rule{0pt}{1.2\normalbaselineskip})', '', '', '', '', '', '', '',
                r"(\cmidrule(lr){2-8})", '', '', '', '', '', '', '',
                r'(\multicolumn{2}{l}{\textbf{Scientific knowledge index}} \rule{0pt}{1.2\normalbaselineskip})', '', '', '', '', '', '', '',
                r"(\cmidrule(lr){2-8})", '', '', '', '', '', '', ''
               )

attr(rows, 'position') <- c(1,2,3,
                            4,9,
                            12,17,
                            20,25,
                            28,33,
                            36,41)

modelsummary(models, stars = TRUE, title = 'Heterogeneity under control, evaluation', add_rows = rows, escape = FALSE) 

modelsummary(models, 
             stars = TRUE,  
             title = 'Heterogeneity under control, evaluation', 
             add_rows = rows,
             escape = FALSE,
             coef_rename = c('Age: Below median (n = 5412)' = r'(\hspace{1em} Below Median)',
                             'Age: Above median (n = 5271)' = r'(\hspace{1em} Above Median)',
                             'Age: Difference' = r'(\hspace{1em} Difference)',
                             'Male:  Not male (n = 5050)' = r'(\hspace{1em} Not male)',
                             'Male: Male (n = 5633)' = r'(\hspace{1em} Male)',
                             'Male: Difference' = r'(\hspace{1em} Difference )',
                             'Political Allegiance:  Not aligned (n = 7570)' = r'(\hspace{1em} Not aligned)',
                             'Political Allegiance: Aligned w/ ruling party (n = 3113)' = r'(\hspace{1em} Aligned)',
                             'Political Allegiance: Difference' = r'(\hspace{1em} Difference  )',
                             'Digital literacy index: Below median (n = 5443)' = r'(\hspace{1em} Below median )',
                             'Digital literacy index: Above median (n = 5240)' = r'(\hspace{1em} Above median )',
                             'Digital literacy index: Difference' = r'(\hspace{1em}  Difference)',
                             'Scientific beliefs index: Below median (n = 5677)' = r'(\hspace{1em} Below median  )',
                             'Scientific beliefs index: Above median (n = 5006)' = r'(\hspace{1em} Above median  )',
                             'Scientific beliefs index: Difference' = r'(\hspace{1em} Difference   )'),
             output = '../tables/heterogeneity_control.tex')

# print out n's
models$Combined$tidy$term
```



### Treatment effect heterogeneity tables
```{r treatment_heterogeneity_primary, cache = TRUE}
# set outcomes
df_eval$pooled_respondent <- rowMeans(aipw_scores[, c('Respondent\nAccuracy', 'Respondent\nFacebook tips')])
df_eval$pooled_headline <- rowMeans(aipw_scores[, c('Headline\nFactcheck', 'Headline\nRelated articles')])

df_eval$pooled_respondent_any_false <- rowMeans(aipw_scores_any_false_eval[, 4:5])
df_eval$pooled_headline_any_false <- rowMeans(aipw_scores_any_false_eval[, 2:3])

df_eval$pooled_respondent_any_true <- rowMeans(aipw_scores_any_true_eval[, 4:5])
df_eval$pooled_headline_any_true <- rowMeans(aipw_scores_any_true_eval[, 2:3])

df_eval$pooled_respondent_send_false <- rowMeans(aipw_scores_send_false_eval[, 4:5])
df_eval$pooled_headline_send_false <- rowMeans(aipw_scores_send_false_eval[, 2:3])

df_eval$pooled_respondent_send_true <- rowMeans(aipw_scores_send_true_eval[, 4:5])
df_eval$pooled_headline_send_true <- rowMeans(aipw_scores_send_true_eval[, 2:3])


df_eval$pooled_respondent_timeline_false <- rowMeans(aipw_scores_timeline_false_eval[, 4:5])
df_eval$pooled_headline_timeline_false <- rowMeans(aipw_scores_timeline_false_eval[, 2:3])

df_eval$pooled_respondent_timeline_true <- rowMeans(aipw_scores_timeline_true_eval[, 4:5])
df_eval$pooled_headline_timeline_true <- rowMeans(aipw_scores_timeline_true_eval[, 2:3])

table <- sapply(covariate_list, function(x) {
  covariate <- x
  outcome <- 'pooled_respondent'
  
  if(length(unique(df_eval[, covariate])) <= 2){
    # binary covariates
    fmla0 <- formula(paste(outcome, ' ~  factor(',covariate, ') -1'))
    fmla1 <- formula(paste(outcome, ' ~  factor(',covariate, ')'))
    ns <- table(df_eval[,covariate])
  }else{
    # continuous covariates
    adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
    fmla0 <- formula(paste0(outcome, ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla1 <- formula(paste0(outcome, ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    ns <- table(df_eval[,covariate] > median(df_eval[,covariate]) + adjustment)
  }
  ols0 <- tidy(lm_robust(fmla0, data=df_eval))
  ols1 <- tidy(lm_robust(fmla1, data=df_eval))
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate, ols1$estimate[2])
  est_std.err <- c(ols0$std.error, ols1$std.error[2])
  p_value <- c(ols0$p.value, ols1$p.value[2])
  ns <- c(ns, sum(ns))
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = ns,
                      `Covariate type` = str_to_sentence(covariate),
                      Type = c(0,1,2),
                      check.names = FALSE)
  
  outdf
  
}, simplify = FALSE)
table <- do.call(rbind, table)

table_combined <- table %>% 
  mutate(
    label = case_when(
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 0 ~ 
        paste0('Below median (n = ', n, ')'),
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 1 ~ 
        paste0('Above median (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 0 ~ 
        paste0(' Not male (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 1 ~ 
        paste0('Male (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 0 ~ 
        paste0('Kenya (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 1 ~ 
        paste0('Nigeria (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 0 ~ 
        paste0(' Not aligned (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 1 ~ 
        paste0('Aligned w/ ruling party (n = ', n, ')'),
      TRUE ~ 'Difference'),
    `Covariate type` = case_when(`Covariate type` == 'Dli' ~ 
                                   'Digital literacy index',
                                 `Covariate type` == 'Hhi' ~ 
                                   'Household Wealth Index',
                                 `Covariate type` == 'Nigeria' ~ 
                                   'Country',
                                 `Covariate type` == 'Pol' ~ 
                                   'Political Allegiance',
                                 `Covariate type` == 'Science' ~ 
                                   'Scientific beliefs index',
                                 TRUE ~ `Covariate type`),
    term = paste0(`Covariate type`, ': ', label)
  )

```


```{r treatment_heterogeneity_any, cache = TRUE}
apply_mat <- expand.grid(stimuli_types, covariate_list)
table <- apply(apply_mat, 1, function(x) {
  stimuli_type <- x[1]
  covariate <- x[2]
  outcome <- paste0('pooled_respondent_any_', stimuli_type)
  
  if(length(unique(df_eval[, covariate])) <= 2){
    # binary covariates
    fmla0 <- formula(paste(outcome, ' ~  factor(',covariate, ') -1'))
    fmla1 <- formula(paste(outcome, ' ~  factor(',covariate, ')'))
    ns <- table(df_eval[,covariate])
  }else{
    # continuous covariates
    adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
    fmla0 <- formula(paste0(outcome, ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla1 <- formula(paste0(outcome, ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    ns <- table(df_eval[,covariate] > median(df_eval[,covariate]) + adjustment)
  }
  ols0 <- tidy(lm_robust(fmla0, data=df_eval))
  ols1 <- tidy(lm_robust(fmla1, data=df_eval))
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate, ols1$estimate[2])
  est_std.err <- c(ols0$std.error, ols1$std.error[2])
  p_value <- c(ols0$p.value, ols1$p.value[2])
  ns <- c(ns, sum(ns))
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = ns,
                      `Stimuli type` = str_to_sentence(stimuli_type),
                      `Covariate type` = str_to_sentence(covariate),
                      Type = c(0,1,2),
                      check.names = FALSE)
  
  outdf
  
})
table <- do.call(rbind, table)

table_any <- table %>% 
  mutate(
    `Stimuli type` = factor(`Stimuli type`, 
                            levels = c('True', 'False')), 
    label = case_when(
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 0 ~ 
        paste0('Below median (n = ', n, ')'),
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 1 ~ 
        paste0('Above median (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 0 ~ 
        paste0(' Not male (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 1 ~ 
        paste0('Male (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 0 ~ 
        paste0('Kenya (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 1 ~ 
        paste0('Nigeria (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 0 ~ 
        paste0(' Not aligned (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 1 ~ 
        paste0('Aligned w/ ruling party (n = ', n, ')'),
      TRUE ~ 'Difference'),
    `Covariate type` = case_when(`Covariate type` == 'Dli' ~ 
                                   'Digital literacy index',
                                 `Covariate type` == 'Hhi' ~ 
                                   'Household Wealth Index',
                                 `Covariate type` == 'Nigeria' ~ 
                                   'Country',
                                 `Covariate type` == 'Pol' ~ 
                                   'Political Allegiance',
                                 `Covariate type` == 'Science' ~ 
                                   'Scientific beliefs index',
                                 TRUE ~ `Covariate type`),
    term = paste0(`Covariate type`, ': ', label)
  )

```

```{r treatment_heterogeneity_channel, cache = TRUE}
apply_mat <- expand.grid(stimuli_types, channel_types, covariate_list)
table <- apply(apply_mat, 1, function(x) {
  stimuli_type <- x[1]
  covariate <- x[3]
  channel <- x[2]
  outcome <- paste0('pooled_respondent_',  channel, '_', stimuli_type)
  
  if(length(unique(df_eval[, covariate])) <= 2){
    # binary covariates
    fmla0 <- formula(paste0(outcome, 
                            ' ~  factor(',covariate, ') -1'))
    fmla1 <- formula(paste0(outcome, 
                            ' ~  factor(',covariate, ')'))
    ns <- table(df_eval[,covariate])
  }else{
    # continuous covariates
    adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
    fmla0 <- formula(paste0(outcome,
                            ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla1 <- formula(paste0(outcome,
                            ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    ns <- table(df_eval[,covariate] > median(df_eval[,covariate]) + adjustment)
  }
  ols0 <- tidy(lm_robust(fmla0, data=df_eval))
  ols1 <- tidy(lm_robust(fmla1, data=df_eval))
  
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate, ols1$estimate[2])
  est_std.err <- c(ols0$std.error, ols1$std.error[2])
  p_value <- c(ols0$p.value, ols1$p.value[2])
  ns <- c(ns, sum(ns))
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = ns,
                      `Stimuli type` = str_to_sentence(stimuli_type),
                      `Covariate type` = str_to_sentence(covariate),
                      `Channel` = channel,
                      Type = c(0,1,2),
                      check.names = FALSE)
  
  outdf
  
})


table <- do.call(rbind, table)

table_channel <- table %>% 
  mutate(
    `Stimuli type` = factor(`Stimuli type`, 
                            levels = c('True', 'False')), 
    label = case_when(
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 0 ~ 
        paste0('Below median (n = ', n, ')'),
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 1 ~ 
        paste0('Above median (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 0 ~ 
        paste0(' Not male (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 1 ~ 
        paste0('Male (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 0 ~ 
        paste0('Kenya (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 1 ~ 
        paste0('Nigeria (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 0 ~ 
        paste0(' Not aligned (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 1 ~ 
        paste0('Aligned w/ ruling party (n = ', n, ')'),
      TRUE ~ 'Difference'),
    `Covariate type` = case_when(`Covariate type` == 'Dli' ~ 
                                   'Digital literacy index',
                                 `Covariate type` == 'Hhi' ~ 
                                   'Household Wealth Index',
                                 `Covariate type` == 'Nigeria' ~ 
                                   'Country',
                                 `Covariate type` == 'Pol' ~ 
                                   'Political Allegiance',
                                 `Covariate type` == 'Science' ~ 
                                   'Scientific beliefs index',
                                 TRUE ~ `Covariate type`),
    term = paste0(`Covariate type`, ': ', label)
  )



```

```{r treatment_heterogeneity_combined, cache = TRUE}
# Mean response estimates
models <- list(
  Combined = list(tidy = table_combined),
  `False: Any sharing` = list(tidy = table_any[which(table_any$`Stimuli type` == 'False'),]),
  `False: Messenger` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'False' &
                                                         table_channel$Channel == 'send'),]),
  `False: Timeline` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'False' &
                                                        table_channel$Channel == 'timeline'),]),
  `True: Any sharing` = list(tidy = table_any[which(table_any$`Stimuli type` == 'True'),]),
  `True: Messenger` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'True' &
                                                        table_channel$Channel == 'send'),]),
  `True: Timeline` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'True' &
                                                       table_channel$Channel == 'timeline'),])
)

models <- lapply(models, function(x){
  class(x) <- "modelsummary_list"
  x
})

################
modelsummary(models, stars = TRUE, title = 'Heterogeneity under pooled respondent treatment, evaluation')
modelsummary(models, stars = TRUE,  title = 'Heterogeneity under pooled respondent treatment, evaluation', 
             output = '../tables/heterogeneity_treatment.tex')

#############
# the new modelsummary
rows <- tribble(~term, ~Combined, ~a, ~b, ~c, ~d, ~e, ~f,
                '', '', '', r'({\textbf{False}})', '', '', r'({\textbf{True}})', '',
                '', r'(\textbf{Combined})', 'Any sharing', 'Messenger', 'Timeline', 'Any sharing', 'Messenger', 'Timeline',
                r'(\cmidrule(lr){2-2} \cmidrule(lr){3-5} \cmidrule(lr){6-8})', '', '', '', '', '', '', '',

                r'(\multicolumn{2}{l}{\textbf{Age}} \rule{0pt}{1.2\normalbaselineskip})', '', '', '', '', '', '', '',
                r"(\cmidrule(lr){2-8})", '', '', '', '', '', '', '',
                r'(\multicolumn{2}{l}{\textbf{Gender}} \rule{0pt}{1.2\normalbaselineskip})', '', '', '', '', '', '', '',
                r"(\cmidrule(lr){2-8})", '', '', '', '', '', '', '',
                r'(\multicolumn{2}{l}{\textbf{Political allegiance}} \rule{0pt}{1.2\normalbaselineskip})', '', '', '', '', '', '', '',
                r"(\cmidrule(lr){2-8})", '', '', '', '', '', '', '',
                r'(\multicolumn{2}{l}{\textbf{Digital literacy index}} \rule{0pt}{1.2\normalbaselineskip})', '', '', '', '', '', '', '',
                r"(\cmidrule(lr){2-8})", '', '', '', '', '', '', '',
                r'(\multicolumn{2}{l}{\textbf{Scientific knowledge index}} \rule{0pt}{1.2\normalbaselineskip})', '', '', '', '', '', '', '',
                r"(\cmidrule(lr){2-8})", '', '', '', '', '', '', ''
               )

attr(rows, 'position') <- c(1,2,3,
                            4,9,
                            12,17,
                            20,25,
                            28,33,
                            36,41)

modelsummary(models, stars = TRUE, title = 'Heterogeneity under pooled respondent treatment, evaluation', add_rows = rows, escape = FALSE) 

modelsummary(models, 
             stars = TRUE,  
             title = 'Heterogeneity under pooled respondent treatment, evaluation', 
             add_rows = rows,
             escape = FALSE,
             coef_rename = c('Age: Below median (n = 5412)' = r'(\hspace{1em} Below Median)',
                             'Age: Above median (n = 5271)' = r'(\hspace{1em} Above Median)',
                             'Age: Difference' = r'(\hspace{1em} Difference)',
                             'Male:  Not male (n = 5050)' = r'(\hspace{1em} Not male)',
                             'Male: Male (n = 5633)' = r'(\hspace{1em} Male)',
                             'Male: Difference' = r'(\hspace{1em} Difference )',
                             'Political Allegiance:  Not aligned (n = 7570)' = r'(\hspace{1em} Not aligned)',
                             'Political Allegiance: Aligned w/ ruling party (n = 3113)' = r'(\hspace{1em} Aligned)',
                             'Political Allegiance: Difference' = r'(\hspace{1em} Difference  )',
                             'Digital literacy index: Below median (n = 5443)' = r'(\hspace{1em} Below median )',
                             'Digital literacy index: Above median (n = 5240)' = r'(\hspace{1em} Above median )',
                             'Digital literacy index: Difference' = r'(\hspace{1em}  Difference)',
                             'Scientific beliefs index: Below median (n = 5677)' = r'(\hspace{1em} Below median  )',
                             'Scientific beliefs index: Above median (n = 5006)' = r'(\hspace{1em} Above median  )',
                             'Scientific beliefs index: Difference' = r'(\hspace{1em} Difference   )'),
             output = '../tables/heterogeneity_treatment.tex')

# print out n's
models$Combined$tidy$term
```
##### By treatment type
```{r accuracy_heterogeneity_primary, cache = TRUE}
covariate_list <- c('age', 'male', 'pol', 'dli', 'science')
stimuli_types <- c('true', 'false')
channel_types <- c('send', 'timeline')

table <- sapply(covariate_list, function(x) {
  covariate <- x
  
  if(length(unique(df_eval[, covariate])) <= 2){
    # binary covariates
    fmla0 <- formula(paste0('aipw_scores[,4]  ~  factor(',covariate, ') -1'))
    fmla1 <- formula(paste0('aipw_scores[,4] ~  factor(',covariate, ')'))
    ns <- table(df_eval[,covariate])
  }else{
    # continuous covariates
    adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
    fmla0 <- formula(paste0('aipw_scores[,4] ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla1 <- formula(paste0('aipw_scores[,4] ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    ns <- table(df_eval[,covariate] > median(df_eval[,covariate]) + adjustment)
  }
  ols0 <- tidy(lm_robust(fmla0, data=df_eval))
  ols1 <- tidy(lm_robust(fmla1, data=df_eval))
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate, ols1$estimate[2])
  est_std.err <- c(ols0$std.error, ols1$std.error[2])
  p_value <- c(ols0$p.value, ols1$p.value[2])
  ns <- c(ns, sum(ns))
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = ns,
                      `Covariate type` = str_to_sentence(covariate),
                      Type = c(0,1,2),
                      check.names = FALSE)
  
  outdf
  
},
simplify = FALSE)
table <- do.call(rbind, table)

table_combined <- table %>% 
  mutate(
    label = case_when(
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 0 ~ 
        paste0('Below median (n = ', n, ')'),
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 1 ~ 
        paste0('Above median (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 0 ~ 
        paste0(' Not male (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 1 ~ 
        paste0('Male (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 0 ~ 
        paste0('Kenya (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 1 ~ 
        paste0('Nigeria (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 0 ~ 
        paste0(' Not aligned (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 1 ~ 
        paste0('Aligned w/ ruling party (n = ', n, ')'),
      TRUE ~ 'Difference'),
    `Covariate type` = case_when(`Covariate type` == 'Dli' ~ 
                                   'Digital literacy index',
                                 `Covariate type` == 'Hhi' ~ 
                                   'Household Wealth Index',
                                 `Covariate type` == 'Nigeria' ~ 
                                   'Country',
                                 `Covariate type` == 'Pol' ~ 
                                   'Political Allegiance',
                                 `Covariate type` == 'Science' ~ 
                                   'Scientific beliefs index',
                                 TRUE ~ `Covariate type`),
    term = paste0(`Covariate type`, ': ', label)
  )

```

```{r accuracy_heterogeneity_any, cache = TRUE}

apply_mat <- expand.grid(stimuli_types, covariate_list)
table <- apply(apply_mat, 1, function(x) {
  stimuli_type <- x[1]
  covariate <- x[2]
  
  if(length(unique(df_eval[, covariate])) <= 2){
    # binary covariates
    fmla0 <- formula(paste0('aipw_scores_any_',stimuli_type,
                            '_eval[,4] ~  factor(',covariate, ') -1'))
    fmla1 <- formula(paste0('aipw_scores_any_',stimuli_type,
                            '_eval[,4] ~  factor(',covariate, ')'))
    ns <- table(df_eval[,covariate])
  }else{
    # continuous covariates
    adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
    fmla0 <- formula(paste0('aipw_scores_any_',stimuli_type, 
                            '_eval[,4] ~  factor(',covariate, 
                            '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla1 <- formula(paste0('aipw_scores_any_',stimuli_type, 
                            '_eval[,4] ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    ns <- table(df_eval[,covariate] > median(df_eval[,covariate]) + adjustment)
  }
  ols0 <- tidy(lm_robust(fmla0, data=df_eval))
  ols1 <- tidy(lm_robust(fmla1, data=df_eval))
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate, ols1$estimate[2])
  est_std.err <- c(ols0$std.error, ols1$std.error[2])
  p_value <- c(ols0$p.value, ols1$p.value[2])
  ns <- c(ns, sum(ns))
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = ns,
                      `Stimuli type` = str_to_sentence(stimuli_type),
                      `Covariate type` = str_to_sentence(covariate),
                      Type = c(0,1,2),
                      check.names = FALSE)
  
  outdf
  
})
table <- do.call(rbind, table)

table_any <- table %>% 
  mutate(
    `Stimuli type` = factor(`Stimuli type`, 
                            levels = c('True', 'False')), 
    label = case_when(
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 0 ~ 
        paste0('Below median (n = ', n, ')'),
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 1 ~ 
        paste0('Above median (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 0 ~ 
        paste0(' Not male (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 1 ~ 
        paste0('Male (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 0 ~ 
        paste0('Kenya (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 1 ~ 
        paste0('Nigeria (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 0 ~ 
        paste0(' Not aligned (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 1 ~ 
        paste0('Aligned w/ ruling party (n = ', n, ')'),
      TRUE ~ 'Difference'),
    `Covariate type` = case_when(`Covariate type` == 'Dli' ~ 
                                   'Digital literacy index',
                                 `Covariate type` == 'Hhi' ~ 
                                   'Household Wealth Index',
                                 `Covariate type` == 'Nigeria' ~ 
                                   'Country',
                                 `Covariate type` == 'Pol' ~ 
                                   'Political Allegiance',
                                 `Covariate type` == 'Science' ~ 
                                   'Scientific beliefs index',
                                 TRUE ~ `Covariate type`),
    term = paste0(`Covariate type`, ': ', label)
  )

```

```{r accuracy_heterogeneity_channel, cache = TRUE}
apply_mat <- expand.grid(stimuli_types, channel_types, covariate_list)
table <- apply(apply_mat, 1, function(x) {
  stimuli_type <- x[1]
  covariate <- x[3]
  channel <- x[2]
  
  if(length(unique(df_eval[, covariate])) <= 2){
    # binary covariates
    fmla0 <- formula(paste0('aipw_scores_', channel, '_', stimuli_type, '_eval[,4]', 
                            
                            ' ~  factor(',covariate, ') -1'))
    fmla1 <- formula(paste0('aipw_scores_any_',stimuli_type, '_eval[,4]', 
                            ' ~  factor(',covariate, ')'))
    ns <- table(df_eval[,covariate])
  }else{
    # continuous covariates
    adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
    fmla0 <- formula(paste0('aipw_scores_', channel, '_',stimuli_type, '_eval[,4]',
                            ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla1 <- formula(paste0('aipw_scores_', channel, '_',stimuli_type, '_eval[,4]',
                            ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    ns <- table(df_eval[,covariate] > median(df_eval[,covariate]) + adjustment)
  }
  ols0 <- tidy(lm_robust(fmla0, data=df_eval))
  ols1 <- tidy(lm_robust(fmla1, data=df_eval))
  
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate, ols1$estimate[2])
  est_std.err <- c(ols0$std.error, ols1$std.error[2])
  p_value <- c(ols0$p.value, ols1$p.value[2])
  ns <- c(ns, sum(ns))
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = ns,
                      `Stimuli type` = str_to_sentence(stimuli_type),
                      `Covariate type` = str_to_sentence(covariate),
                      `Channel` = channel,
                      Type = c(0,1,2),
                      check.names = FALSE)
  
  outdf
  
})


table <- do.call(rbind, table)

table_channel <- table %>% 
  mutate(
    `Stimuli type` = factor(`Stimuli type`, 
                            levels = c('True', 'False')), 
    label = case_when(
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 0 ~ 
        paste0('Below median (n = ', n, ')'),
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 1 ~ 
        paste0('Above median (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 0 ~ 
        paste0(' Not male (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 1 ~ 
        paste0('Male (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 0 ~ 
        paste0('Kenya (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 1 ~ 
        paste0('Nigeria (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 0 ~ 
        paste0(' Not aligned (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 1 ~ 
        paste0('Aligned w/ ruling party (n = ', n, ')'),
      TRUE ~ 'Difference'),
    `Covariate type` = case_when(`Covariate type` == 'Dli' ~ 
                                   'Digital literacy index',
                                 `Covariate type` == 'Hhi' ~ 
                                   'Household Wealth Index',
                                 `Covariate type` == 'Nigeria' ~ 
                                   'Country',
                                 `Covariate type` == 'Pol' ~ 
                                   'Political Allegiance',
                                 `Covariate type` == 'Science' ~ 
                                   'Scientific beliefs index',
                                 TRUE ~ `Covariate type`),
    term = paste0(`Covariate type`, ': ', label)
  )



```

```{r accuracy_combined_heterogeneity, cache = TRUE}
# Mean response estimates
models <- list(
  Combined = list(tidy = table_combined),
  `False: Any sharing` = list(tidy = table_any[which(table_any$`Stimuli type` == 'False'),]),
  `False: Messenger` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'False' &
                                                         table_channel$Channel == 'send'),]),
  `False: Timeline` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'False' &
                                                        table_channel$Channel == 'timeline'),]),
  `True: Any sharing` = list(tidy = table_any[which(table_any$`Stimuli type` == 'True'),]),
  `True: Messenger` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'True' &
                                                        table_channel$Channel == 'send'),]),
  `True: Timeline` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'True' &
                                                       table_channel$Channel == 'timeline'),])
)

models <- lapply(models, function(x){
  class(x) <- "modelsummary_list"
  x
})

modelsummary(models, stars = TRUE, title = 'Heterogeneity under accuracy, evaluation')
modelsummary(models, stars = TRUE,  title = 'Heterogeneity under accuracy, evaluation', 
             output = '../tables/heterogeneity_accuracy.tex')
```

#### Facebook
```{r facebook_heterogeneity_primary, cache = TRUE}
covariate_list <- c('age', 'male', 'pol', 'dli', 'science')
stimuli_types <- c('true', 'false')
channel_types <- c('send', 'timeline')

table <- sapply(covariate_list, function(x) {
  covariate <- x
  
  if(length(unique(df_eval[, covariate])) <= 2){
    # binary covariates
    fmla0 <- formula(paste0('aipw_scores[,5]  ~  factor(',covariate, ') -1'))
    fmla1 <- formula(paste0('aipw_scores[,5] ~  factor(',covariate, ')'))
    ns <- table(df_eval[,covariate])
  }else{
    # continuous covariates
    adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
    fmla0 <- formula(paste0('aipw_scores[,5] ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla1 <- formula(paste0('aipw_scores[,5] ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    ns <- table(df_eval[,covariate] > median(df_eval[,covariate]) + adjustment)
  }
  ols0 <- tidy(lm_robust(fmla0, data=df_eval))
  ols1 <- tidy(lm_robust(fmla1, data=df_eval))
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate, ols1$estimate[2])
  est_std.err <- c(ols0$std.error, ols1$std.error[2])
  p_value <- c(ols0$p.value, ols1$p.value[2])
  ns <- c(ns, sum(ns))
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = ns,
                      `Covariate type` = str_to_sentence(covariate),
                      Type = c(0,1,2),
                      check.names = FALSE)
  
  outdf
  
},
simplify = FALSE)
table <- do.call(rbind, table)

table_combined <- table %>% 
  mutate(
    label = case_when(
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 0 ~ 
        paste0('Below median (n = ', n, ')'),
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 1 ~ 
        paste0('Above median (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 0 ~ 
        paste0(' Not male (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 1 ~ 
        paste0('Male (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 0 ~ 
        paste0('Kenya (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 1 ~ 
        paste0('Nigeria (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 0 ~ 
        paste0(' Not aligned (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 1 ~ 
        paste0('Aligned w/ ruling party (n = ', n, ')'),
      TRUE ~ 'Difference'),
    `Covariate type` = case_when(`Covariate type` == 'Dli' ~ 
                                   'Digital literacy index',
                                 `Covariate type` == 'Hhi' ~ 
                                   'Household Wealth Index',
                                 `Covariate type` == 'Nigeria' ~ 
                                   'Country',
                                 `Covariate type` == 'Pol' ~ 
                                   'Political Allegiance',
                                 `Covariate type` == 'Science' ~ 
                                   'Scientific beliefs index',
                                 TRUE ~ `Covariate type`),
    term = paste0(`Covariate type`, ': ', label)
  )

```

```{r facebook_heterogeneity_any, cache = TRUE}

apply_mat <- expand.grid(stimuli_types, covariate_list)
table <- apply(apply_mat, 1, function(x) {
  stimuli_type <- x[1]
  covariate <- x[2]
  
  if(length(unique(df_eval[, covariate])) <= 2){
    # binary covariates
    fmla0 <- formula(paste0('aipw_scores_any_',stimuli_type,
                            '_eval[,5] ~  factor(',covariate, ') -1'))
    fmla1 <- formula(paste0('aipw_scores_any_',stimuli_type,
                            '_eval[,5] ~  factor(',covariate, ')'))
    ns <- table(df_eval[,covariate])
  }else{
    # continuous covariates
    adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
    fmla0 <- formula(paste0('aipw_scores_any_',stimuli_type, 
                            '_eval[,5] ~  factor(',covariate, 
                            '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla1 <- formula(paste0('aipw_scores_any_',stimuli_type, 
                            '_eval[,5] ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    ns <- table(df_eval[,covariate] > median(df_eval[,covariate]) + adjustment)
  }
  ols0 <- tidy(lm_robust(fmla0, data=df_eval))
  ols1 <- tidy(lm_robust(fmla1, data=df_eval))
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate, ols1$estimate[2])
  est_std.err <- c(ols0$std.error, ols1$std.error[2])
  p_value <- c(ols0$p.value, ols1$p.value[2])
  ns <- c(ns, sum(ns))
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = ns,
                      `Stimuli type` = str_to_sentence(stimuli_type),
                      `Covariate type` = str_to_sentence(covariate),
                      Type = c(0,1,2),
                      check.names = FALSE)
  
  outdf
  
})
table <- do.call(rbind, table)

table_any <- table %>% 
  mutate(
    `Stimuli type` = factor(`Stimuli type`, 
                            levels = c('True', 'False')), 
    label = case_when(
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 0 ~ 
        paste0('Below median (n = ', n, ')'),
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 1 ~ 
        paste0('Above median (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 0 ~ 
        paste0(' Not male (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 1 ~ 
        paste0('Male (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 0 ~ 
        paste0('Kenya (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 1 ~ 
        paste0('Nigeria (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 0 ~ 
        paste0(' Not aligned (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 1 ~ 
        paste0('Aligned w/ ruling party (n = ', n, ')'),
      TRUE ~ 'Difference'),
    `Covariate type` = case_when(`Covariate type` == 'Dli' ~ 
                                   'Digital literacy index',
                                 `Covariate type` == 'Hhi' ~ 
                                   'Household Wealth Index',
                                 `Covariate type` == 'Nigeria' ~ 
                                   'Country',
                                 `Covariate type` == 'Pol' ~ 
                                   'Political Allegiance',
                                 `Covariate type` == 'Science' ~ 
                                   'Scientific beliefs index',
                                 TRUE ~ `Covariate type`),
    term = paste0(`Covariate type`, ': ', label)
  )

```

```{r facebook_heterogeneity_channel, cache = TRUE}
apply_mat <- expand.grid(stimuli_types, channel_types, covariate_list)
table <- apply(apply_mat, 1, function(x) {
  stimuli_type <- x[1]
  covariate <- x[3]
  channel <- x[2]
  
  if(length(unique(df_eval[, covariate])) <= 2){
    # binary covariates
    fmla0 <- formula(paste0('aipw_scores_', channel, '_', stimuli_type, '_eval[,5]', 
                            
                            ' ~  factor(',covariate, ') -1'))
    fmla1 <- formula(paste0('aipw_scores_any_',stimuli_type, '_eval[,5]', 
                            ' ~  factor(',covariate, ')'))
    ns <- table(df_eval[,covariate])
  }else{
    # continuous covariates
    adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
    fmla0 <- formula(paste0('aipw_scores_', channel, '_',stimuli_type, '_eval[,5]',
                            ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla1 <- formula(paste0('aipw_scores_', channel, '_',stimuli_type, '_eval[,5]',
                            ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    ns <- table(df_eval[,covariate] > median(df_eval[,covariate]) + adjustment)
  }
  ols0 <- tidy(lm_robust(fmla0, data=df_eval))
  ols1 <- tidy(lm_robust(fmla1, data=df_eval))
  
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate, ols1$estimate[2])
  est_std.err <- c(ols0$std.error, ols1$std.error[2])
  p_value <- c(ols0$p.value, ols1$p.value[2])
  ns <- c(ns, sum(ns))
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = ns,
                      `Stimuli type` = str_to_sentence(stimuli_type),
                      `Covariate type` = str_to_sentence(covariate),
                      `Channel` = channel,
                      Type = c(0,1,2),
                      check.names = FALSE)
  
  outdf
  
})


table <- do.call(rbind, table)

table_channel <- table %>% 
  mutate(
    `Stimuli type` = factor(`Stimuli type`, 
                            levels = c('True', 'False')), 
    label = case_when(
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 0 ~ 
        paste0('Below median (n = ', n, ')'),
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 1 ~ 
        paste0('Above median (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 0 ~ 
        paste0(' Not male (n = ', n, ')'),
      `Covariate type` %in%  c('Male')  & 
        Type == 1 ~ 
        paste0('Male (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 0 ~ 
        paste0('Kenya (n = ', n, ')'),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 1 ~ 
        paste0('Nigeria (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 0 ~ 
        paste0(' Not aligned (n = ', n, ')'),
      `Covariate type` %in%  c('Pol')  & 
        Type == 1 ~ 
        paste0('Aligned w/ ruling party (n = ', n, ')'),
      TRUE ~ 'Difference'),
    `Covariate type` = case_when(`Covariate type` == 'Dli' ~ 
                                   'Digital literacy index',
                                 `Covariate type` == 'Hhi' ~ 
                                   'Household Wealth Index',
                                 `Covariate type` == 'Nigeria' ~ 
                                   'Country',
                                 `Covariate type` == 'Pol' ~ 
                                   'Political Allegiance',
                                 `Covariate type` == 'Science' ~ 
                                   'Scientific beliefs index',
                                 TRUE ~ `Covariate type`),
    term = paste0(`Covariate type`, ': ', label)
  )



```

```{r facebook_combined_heterogeneity, cache = TRUE}
# Mean response estimates
models <- list(
  Combined = list(tidy = table_combined),
  `False: Any sharing` = list(tidy = table_any[which(table_any$`Stimuli type` == 'False'),]),
  `False: Messenger` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'False' &
                                                         table_channel$Channel == 'send'),]),
  `False: Timeline` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'False' &
                                                        table_channel$Channel == 'timeline'),]),
  `True: Any sharing` = list(tidy = table_any[which(table_any$`Stimuli type` == 'True'),]),
  `True: Messenger` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'True' &
                                                        table_channel$Channel == 'send'),]),
  `True: Timeline` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'True' &
                                                       table_channel$Channel == 'timeline'),])
)

models <- lapply(models, function(x){
  class(x) <- "modelsummary_list"
  x
})

modelsummary(models, stars = TRUE, title = 'Heterogeneity under facebook, evaluation')
modelsummary(models, stars = TRUE,  title = 'Heterogeneity under facebook, evaluation', 
             output = '../tables/heterogeneity_facebook.tex')
```


### Policy heterogeneity tables
```{r policy_heterogeneity_combined, cache = TRUE}

combined_df <- data.frame(
  combined = c(aipw_scores[,4] + aipw_scores[,1],
               aipw_scores[,5] + aipw_scores[,1]),
  condition = rep(4:5, each = nrow(df_eval)),
  optimal = rep(optimal_assignment, 2)
)


table <- {
  outcome <- 'combined'
  
  fmla0 <- formula(paste(outcome, ' ~  factor(condition)*factor(optimal) -1'))
  fmla1 <- formula(paste(outcome, ' ~  factor(condition)*factor(optimal)'))
  fmla2 <- formula(paste(outcome, ' ~  factor(condition)*factor(optimal==4) -1'))
  fmla3 <- formula(paste(outcome, ' ~  factor(condition)*factor(optimal==4)'))
  ns <- table(optimal_assignment)
  ols0 <- tidy(lm_robust(fmla0, data=combined_df))
  ols1 <- tidy(lm_robust(fmla1, data=combined_df))
  ols2 <- tidy(lm_robust(fmla2, data=combined_df))
  ols3 <- tidy(lm_robust(fmla3, data=combined_df))
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate[1:2], ols1$estimate[2],
                ols2$estimate[1:2], ols3$estimate[c(2,4)])
  est_std.err <- c(ols0$std.error[1:2], ols1$std.error[2],
                   ols2$std.error[1:2], ols3$std.error[c(2,4)])
  p_value <- c(NA, NA, ols1$p.value[2],
               NA, NA, ols3$p.value[c(2,4)])
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = c(rep(ns, each = 3), sum(ns)),
                      `Policy type` = c(rep('Accuracy nudge', 3), rep('Facebook tips', 3), 'Grand difference'),
                      Type = c(rep(0:2, times = 2), 3),
                      check.names = FALSE)
  
  outdf
  
}

table_combined <- table %>% 
  mutate(
    term = paste0(`Policy type`, ': ', 
                  case_when(Type == 0 ~ 'Accuracy nudge',
                            Type == 1 ~ 'Facebook tips',
                            Type == 2 ~ 'Difference',
                            TRUE ~ 'Grand difference'))
  )

```

```{r policy_heterogeneity_any, cache = TRUE}

any_df <- data.frame(
  any_false = c(aipw_scores_any_false_eval[,4] + aipw_scores_any_false_eval[,1],
                aipw_scores_any_false_eval[,5] + aipw_scores_any_false_eval[,1]),
  any_true = c(aipw_scores_any_true_eval[,4] + aipw_scores_any_true_eval[,1],
               aipw_scores_any_true_eval[,5] + aipw_scores_any_true_eval[,1]),
  condition = rep(4:5, each = nrow(df_eval)),
  optimal = rep(optimal_assignment, 2)
)

apply_mat <- expand.grid(stimuli_types)

table <- apply(apply_mat, 1, function(x) {
  stimuli_type <- x[1]
  outcome <- paste0('any_', stimuli_type)
  
  fmla0 <- formula(paste(outcome, ' ~  factor(condition)*factor(optimal) -1'))
  fmla1 <- formula(paste(outcome, ' ~  factor(condition)*factor(optimal)'))
  fmla2 <- formula(paste(outcome, ' ~  factor(condition)*factor(optimal==4) -1'))
  fmla3 <- formula(paste(outcome, ' ~  factor(condition)*factor(optimal==4)'))
  ns <- table(optimal_assignment)
  ols0 <- tidy(lm_robust(fmla0, data=any_df))
  ols1 <- tidy(lm_robust(fmla1, data=any_df))
  ols2 <- tidy(lm_robust(fmla2, data=any_df))
  ols3 <- tidy(lm_robust(fmla3, data=any_df))
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate[1:2], ols1$estimate[2],
                ols2$estimate[1:2], ols3$estimate[c(2,4)])
  est_std.err <- c(ols0$std.error[1:2], ols1$std.error[2],
                   ols2$std.error[1:2], ols3$std.error[c(2,4)])
  p_value <- c(NA, NA, ols1$p.value[2],
               NA, NA, ols3$p.value[c(2,4)])
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = c(rep(ns, each = 3), sum(ns)),
                      `Stimuli type` = str_to_sentence(stimuli_type),
                      `Policy type` = c(rep('Accuracy nudge', 3), rep('Facebook tips', 3), 'Grand difference'),
                      Type = c(rep(0:2, times = 2), 3),
                      check.names = FALSE)
  
  outdf
  
})

table <- do.call(rbind, table)

table_any <- table %>% 
  mutate(
    `Stimuli type` = factor(`Stimuli type`, 
                            levels = c('True', 'False')), 
    term = paste0(`Policy type`, ': ', 
                  case_when(Type == 0 ~ 'Accuracy nudge',
                            Type == 1 ~ 'Facebook tips',
                            Type == 2 ~ 'Difference',
                            TRUE ~ 'Grand difference'))
  )

```




```{r policy_heterogeneity_channel, cache = TRUE}
apply_mat <- expand.grid(stimuli_types, channel_types)

channel_df <- data.frame(
  channel_false_send = c(aipw_scores_send_false_eval[,4] + aipw_scores_send_false_eval[,1],
                         aipw_scores_send_false_eval[,5] + aipw_scores_send_false_eval[,1]),
  channel_false_timeline = c(aipw_scores_timeline_false_eval[,4] + aipw_scores_timeline_false_eval[,1],
                             aipw_scores_timeline_false_eval[,5] + aipw_scores_timeline_false_eval[,1]),
  channel_true_send = c(aipw_scores_send_true_eval[,4] + aipw_scores_send_true_eval[,1],
                        aipw_scores_send_true_eval[,5] + aipw_scores_send_true_eval[,1]),
  channel_true_timeline = c(aipw_scores_timeline_true_eval[,4] + aipw_scores_timeline_true_eval[,1],
                            aipw_scores_timeline_true_eval[,5] + aipw_scores_timeline_true_eval[,1]),
  condition = rep(4:5, each = nrow(df_eval)),
  optimal = rep(optimal_assignment, 2)
)

table <- apply(apply_mat, 1, function(x) {
  stimuli_type <- x[1]
  channel <- x[2]
  outcome <- paste0('channel_', stimuli_type, '_', channel)
  
  fmla0 <- formula(paste(outcome, ' ~  factor(condition)*factor(optimal) -1'))
  fmla1 <- formula(paste(outcome, ' ~  factor(condition)*factor(optimal)'))
  fmla2 <- formula(paste(outcome, ' ~  factor(condition)*factor(optimal==4) -1'))
  fmla3 <- formula(paste(outcome, ' ~  factor(condition)*factor(optimal==4)'))
  ns <- table(optimal_assignment)
  ols0 <- tidy(lm_robust(fmla0, data=channel_df))
  ols1 <- tidy(lm_robust(fmla1, data=channel_df))
  ols2 <- tidy(lm_robust(fmla2, data=channel_df))
  ols3 <- tidy(lm_robust(fmla3, data=channel_df))
  
  # coef, standard error, n
  est_coef <- c(ols0$estimate[1:2], ols1$estimate[2],
                ols2$estimate[1:2], ols3$estimate[c(2,4)])
  est_std.err <- c(ols0$std.error[1:2], ols1$std.error[2],
                   ols2$std.error[1:2], ols3$std.error[c(2,4)])
  p_value <- c(NA, NA, ols1$p.value[2],
               NA, NA, ols3$p.value[c(2,4)])
  
  # Tally up results
  outdf <- data.frame(estimate = est_coef,
                      std.error = est_std.err, 
                      p.value = p_value,
                      n = c(rep(ns, each = 3), sum(ns)),
                      `Stimuli type` = str_to_sentence(stimuli_type),
                      `Channel` = channel,
                      `Policy type` = c(rep('Accuracy nudge', 3), rep('Facebook tips', 3), 'Grand difference'),
                      Type = c(rep(0:2, times = 2), 3),
                      check.names = FALSE)
  
  outdf
  
})

table <- do.call(rbind, table)

table_channel <- table %>% 
  mutate(
    `Stimuli type` = factor(`Stimuli type`, 
                            levels = c('True', 'False')), 
    term = paste0(`Policy type`, ': ', 
                  case_when(Type == 0 ~ 'Accuracy nudge',
                            Type == 1 ~ 'Facebook tips',
                            Type == 2 ~ 'Difference',
                            TRUE ~ 'Grand difference'))
  )
```

```{r policy_heterogeneity_combined_tables, cache = TRUE}
# Mean response estimates
models <- list(
  Combined = list(tidy = table_combined), 
  `False: Any sharing` = list(tidy = table_any[which(table_any$`Stimuli type` == 'False'),]),
  `False: Messenger` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'False' &
                                                         table_channel$Channel == 'send'),]),
  `False: Timeline` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'False' &
                                                        table_channel$Channel == 'timeline'),]),
  `True: Any sharing` = list(tidy = table_any[which(table_any$`Stimuli type` == 'True'),]),
  `True: Messenger` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'True' &
                                                        table_channel$Channel == 'send'),]),
  `True: Timeline` = list(tidy = table_channel[which(table_channel$`Stimuli type` == 'True' &
                                                       table_channel$Channel == 'timeline'),])
)

models <- lapply(models, function(x){
  class(x) <- "modelsummary_list"
  x
})

modelsummary(models, stars = TRUE, title = 'Policy heterogeneity across respondent treatments, evaluation')
modelsummary(models, stars = TRUE,  title = 'Policy heterogeneity under pooled respondent treatment, evaluation', 
             output = '../tables/heterogeneity_policy.tex')

```

# Secondary analysis
*	### Variation based on scientific views, cognitive reflection test
+	Notes on analysis:
+ Outcomes:
1. ) Sharing intentions as formalized in primary response function (Y =−Mb+0.5Tb, pre-analysis plan p. 17)
2. ) Sharing intentions for true/false stimuli separately (as proportion of stimuli of each type seen)
- Use learning data
- Replicate Figure 4
![image](https://user-images.githubusercontent.com/77539474/193426695-a7503a06-1024-4b34-bfb2-5e24fab2a7d1.png)
![image](https://user-images.githubusercontent.com/77539474/193426702-45428e07-0c7d-4fcd-aecb-56b166206660.png)


+ Hypothesis: 
-  Facebook tips or AfricaCheck tips > Accuracy for respondents with high DLI, high CRT, high Science 


* ### Industry Practice
+ Notes on analysis:
+ Outcomes:
1. ) Sharing intentions as formalized in primary response function (Y =−Mb+0.5Tb, pre-analysis plan p. 17)
2. ) Sharing intentions for true/false stimuli separately (as proportion of stimuli of each type seen)
- Use learning data
- Compare response under the following treatments to indicator = 1 or ≥ median to indicator = 0 or < median
- Treatments
- Factcheck (headline)
- More information (headline)
- Related articles (headline)
- Facebook tips (respondent)
- AfricaCheck tips (respondent) 
- Covariates
- Age
- Male
- Education
+ Hypotheses:
-  The effect of Factcheck, more information, related articles: more educated users, older people, and women > less educated, younger and male 
-  The effect (reduce sharing of misinformation) of Facebook tips, AfricaCheck tips: less-educated > those with more education


* ### Social Science Theory
+ Notes on analysis
+ Outcomes:
1. ) Sharing intentions as formalized in primary response function (Y =−Mb+0.5Tb, pre-analysis plan p. 17)
2. ) Sharing intentions for true/false stimuli separately (as proportion of stimuli of each type seen)
- Use learning data
- Compare response under the following treatments to indicator = 1 or ≥ median to indicator = 0 or < median
- Covariates:
- CRT
- Facebook usage
- Religiosity
- Treatments:
- Accuracy nudge (respondent)
- Deliberation nudge (respondent)
- Pledge (respondent)
+ Hypotheses
-  Compare responses under accuracy nudge and deliberation nudge between low CRT and high CRT respondents
-  Pledge respondent-level treatment: frequent user of Facebook, more religious, and high CRT > less frequent user of Facebook, less religious, and low CRT


* ### Best respondent and headline-level treatments
+ Notes on analysis
+ Outcomes:
1. ) Sharing intentions as formalized in primary response function (Y =−Mb+0.5Tb, pre-analysis plan p. 17)
2. ) Sharing intentions for true/false stimuli separately (as proportion of stimuli of each type seen)
- Use evaluation data
+ Hypotheses
-  How locus of control and age interact with the best uniform respondent-level treatment
-  How CRT and education interact with the best uniform headline-level treatment


* ### Baseline level
+ Notes on analysis
+ Outcomes:
1. ) Sharing intentions as formalized in primary response function (Y =−Mb+0.5Tb, pre-analysis plan p. 17)
2. ) Sharing intentions for true/false stimuli separately (as proportion of stimuli of each type seen)
- Compare response under the following covariates to indicator = 1 or ≥ median to indicator = 0 or < median
- Use evaluation data
+ Hypotheses
- Certain types of people are simply more likely to share false information:
-  Young
-  Male
-  Less educated
-  Low CRT
-  More religious

#### Heterogeneity figure
```{r pooled_respondent_any_on_covariates_figure, cache = TRUE}

covariate_list <- c('male', 'science', 'age', 'dli', 'pol')
stimuli_types <- c('true', 'false')

df_eval$pooled_respondent_any_false <- rowMeans(aipw_scores_any_false_eval[, 4:5])
df_eval$pooled_respondent_any_true <- rowMeans(aipw_scores_any_true_eval[, 4:5])


apply_mat <- expand.grid(stimuli_types, covariate_list)
table <- apply(apply_mat, 1, function(x) {
  stimuli_type <- x[1]
  covariate <- x[2]
  outcome <- paste0('pooled_respondent_any_', stimuli_type)
  
  if(length(unique(df_eval[, covariate])) <= 2){
    # binary covariates
    fmla0 <- formula(paste(outcome, ' ~  factor(',covariate, ') -1'))
    fmla1 <- formula(paste(outcome, ' ~  factor(',covariate, ')'))
    fmla0_0 <- formula(paste0('aipw_scores_any_',stimuli_type,'_eval[,1] ~  factor(',covariate, ') -1'))
    fmla0_1 <- formula(paste0('aipw_scores_any_',stimuli_type,'_eval[,1] ~  factor(',covariate, ')'))
    ns <- table(df_eval[,covariate])
  }else{
    # continuous covariates
    adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
    fmla0 <- formula(paste0(outcome, ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla1 <- formula(paste0(outcome, ' ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    fmla0_0 <- formula(paste0('aipw_scores_any_',stimuli_type,'_eval[,1] ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
    fmla0_1 <- formula(paste0('aipw_scores_any_',stimuli_type,'_eval[,1] ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,')'))
    ns <- table(df_eval[,covariate] > median(df_eval[,covariate]) + adjustment)
  }
  
  ols0 <- tidy(lm_robust(fmla0, data=df_eval)) # treatment effect means by group
  ols1 <- tidy(lm_robust(fmla1, data=df_eval)) # difference in treatment effects
  ols0_0 <- tidy(lm_robust(fmla0_0, data=df_eval)) # control means by group
  ols0_1 <- tidy(lm_robust(fmla0_1, data=df_eval)) # difference in control
  
  # coef, standardm error, p value
  est_coef <- round(c(ols0_0$estimate, 
                      ols0$estimate + ols0_0$estimate), 3)
  est_std.err <- round(c(ols0_0$std.error, 
                         ols0$std.error), 3)
  p_vals <- c(paste0(round(ols0_1$estimate, 3), '\n', '(', 
                     round(ols0_1$std.error, 3), ')')[2:length(ols0_1$estimate)],
              paste0(round(ols0$estimate, 3), '\n', '(', 
                     round(ols0$std.error, 3), ')'),
              paste0(round(ols1$estimate, 3), '\n', '(', 
                     round(ols1$std.error, 3), ')')[2:length(ols1$estimate)])
  
  
  # Tally up results
  outdf <- data.frame(stringr::str_to_sentence(stimuli_type),
                      stringr::str_to_sentence(covariate), 
                      rep(c('Control', 'Pooled\nrespondent'), 
                          each = length(ols0_0$estimate)),
                      rep(
                        if(length(ols0_0$estimate) == 2){
                          c('Not/lower type', 'Type/higher type')
                        } else {
                          paste0('Level ', sort(unique(df_eval[, covariate]))) 
                        },
                        times = 2),
                      est_coef, est_std.err, matrix(p_vals, nrow = 1), 
                      rep(prettyNum(ns, big.mark = ","), times = 2))
  colnames(outdf) <- c('Stimuli type', 'Covariate type', 
                       'Condition', 'Type',
                       'estimate', 'std.error', 
                       paste0(
                         'p-value for control difference',
                         if(length(ols0_0$estimate) == 2){
                           c('')
                         } else {
                           paste0(' ', sort(unique(df_eval[, covariate]))[-1])
                         }
                       ),
                       paste0(
                         'p-value for te difference ',
                         if(length(ols0_0$estimate) == 2){
                           c('low', 'high')
                         } else {
                           sort(unique(df_eval[, covariate])) 
                         }
                       ),
                       paste0(
                         'p-value for diff in diff',
                         if(length(ols0_0$estimate) == 2){
                           c('')
                         } else {
                           paste0(' ' , sort(unique(df_eval[, covariate]))[-1])
                         }
                       ), 
                       'n')
  outdf
  
})
table <- do.call(plyr::rbind.fill, table)


table <- table %>% 
  mutate(
    `Stimuli type` = factor(`Stimuli type`, 
                            levels = c('True', 'False')), 
    label = case_when(
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type == 'Not/lower type' ~ 
        paste0('< median\nn = ', n),
      `Covariate type` %in%  c('Age', 'Dli','Hhi', 'Science')  & 
        Type != 'Not/lower type' ~ 
        paste0('≥ median\nn = ', n),
      `Covariate type` %in%  c('Male')  & 
        Type == 'Not/lower type' ~ 
        paste0(' Not male\nn = ', n),
      `Covariate type` %in%  c('Male')  & 
        Type != 'Not/lower type' ~ 
        paste0('Male\nn = ', n),
      `Covariate type` %in%  c('Nigeria')  & 
        Type == 'Not/lower type' ~ 
        paste0('Kenya\nn = ', n),
      `Covariate type` %in%  c('Nigeria')  & 
        Type != 'Not/lower type' ~ 
        paste0('Nigeria\nn = ', n),
      `Covariate type` %in%  c('Pol')  & 
        Type == 'Not/lower type' ~ 
        paste0(' Not aligned\nn = ', n),
      `Covariate type` %in%  c('Pol')  & 
        Type != 'Not/lower type' ~ 
        paste0('Aligned w/\nruling party\nn = ', n)),
    `Covariate type` = case_when(`Covariate type` == 'Dli' ~ 
                                   'Digital\nLiteracy\nIndex',
                                 `Covariate type` == 'Hhi' ~ 
                                   'Household\nWealth\nIndex',
                                 `Covariate type` == 'Nigeria' ~ 
                                   'Country',
                                 `Covariate type` == 'Pol' ~ 
                                   'Political\nAllegiance',
                                 `Covariate type` == 'Science' ~ 
                                   'Scientific\nBeliefs\nIndex',
                                 TRUE ~ `Covariate type`)
  )


table_stats <- table %>% 
  group_by(`Stimuli type`, `Covariate type`) %>% 
  mutate(y = max(estimate)) %>% 
  group_by(`Stimuli type`, `Covariate type`, Condition) %>% 
  mutate(
    p = unique(`p-value for diff in diff`),
    p_low = unique(`p-value for te difference low`),
    p_high = unique(`p-value for te difference high`),
    p0 = unique(`p-value for control difference`),
    y_c = max(estimate), 
    x = 1,
    xend = 2) %>%
  group_by(`Stimuli type`, `Covariate type`, Condition, Type) %>% 
  mutate(
    y_t = max(estimate),
  ) %>% 
  filter(Condition == 'Control')

# only one row per facet plot
table_stats0 <- table_stats[which(table_stats$Type == 'Not/lower type'),]

p <- ggplot(table, 
            aes(y = estimate, x = label, group = Condition, 
                fill = `Stimuli type`)) +
  geom_col(aes(alpha = Condition),
           position = position_dodge(width=0.5), size = 0) +
  facet_grid(rows = vars(`Stimuli type`), 
             cols = vars(`Covariate type`), scales = 'free_x') +
  theme_minimal() + 
  coord_cartesian(ylim = c(0.3, 0.9)) + 
  scale_alpha_manual(values = c(0.6, 1)) + 
  scale_fill_manual(values = cbPalette[c(6,7)]) +
  theme(
    plot.title = element_text(hjust = 0.5),
    panel.background = element_rect(fill = 'white'),
    plot.background = element_rect(fill = 'white', color = 'white'))+
  scale_x_discrete(guide=guide_axis(n.dodge=2)) +
  xlab('Covariate types') + 
  ylab('Estimate')



p +
  stat_pointinterval(aes(x = label,
                         ydist = distributional::dist_normal(estimate, std.error), alpha = Condition),
                     position = position_dodge(width=0.5),
                     .width = c(.66, .95, .99),
                     point_size = 0, color = 'grey60') + 
  scale_alpha_manual(values = c(0.6, 0)) + 
  theme(legend.text=element_text(color="white"),
        legend.title = element_text(color = 'white')) + 
  guides(alpha = guide_legend(
    # title.theme = element_text(color = 'black'),
    # label.theme = element_text(color = 'black'),
    override.aes = list(alpha = 0)),
    fill = guide_legend(override.aes = list(alpha = 0.6),
                        title.theme = element_text(color = 'black', size = 11),
                        label.theme = element_text(color = 'black', size = 9)
    )) +
  # flat bar
  geom_segment(data = table_stats0, 
               aes(x = x-.125, xend = xend-.125,
                   y = y_c + .1, 
                   yend = y_c + .1), lwd = .5, 
               color = 'grey60') + 
  # left leg
  geom_segment(data = table_stats0, 
               aes(x = x-.125, xend = x-.125,
                   y = y_c + .06, 
                   yend = y_c + .1), lwd = .5, 
               color = 'grey60') + 
  # right leg
  geom_segment(data = table_stats0, 
               aes(x = xend-.125, xend = xend-.125,
                   y = y_c + .06, 
                   yend = y_c + .1), lwd = .5, 
               color = 'grey60') +  
  # center notch
  geom_segment(data = table_stats0, 
               aes(x = 1.375, xend = 1.375,
                   y = y_c + .09, 
                   yend = y_c + .11), lwd = .5, 
               color = 'grey60') + 
  geom_text(data = table_stats0, 
            aes(x = 1.375,
                y = y_c + .22, label = p0), 
            color = 'grey30', size = 3) +
  coord_cartesian(ylim = c(0,1)) + 
  labs(title = 'Average response, true and false sharing, control condition',
       subtitle = 'Evaluation data'
       # ,
       # caption = 'P-values represent statistical significance of difference across covariate types in control response estimates.'
  ) +
  ylab('Estimate')

ggsave('../figures/any_sharing_by_covariates_control.png', height = 7, width = 9.5)



p +
  stat_pointinterval(aes(x = label,
                         ydist = distributional::dist_normal(estimate, std.error)),
                     position = position_dodge(width=0.5),
                     .width = c(.66, .95, .99),
                     point_size = 0,
                     color = 'grey60') + 
  # flat bar
  geom_segment(data = table_stats, 
               aes(x = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type')-.125, 
                   xend = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type')+.125,
                   y = y_t + .1, 
                   yend = y_t + .1), lwd = .5, 
               color = 'grey60') + 
  # left leg
  geom_segment(data = table_stats, 
               aes(x = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type')-.125, 
                   xend = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type')-.125,
                   y = y_t + .06, 
                   yend = y_t + .1), lwd = .5, 
               color = 'grey60') + 
  # right leg
  geom_segment(data = table_stats, 
               aes(x = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type')+.125, 
                   xend = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type')+.125,
                   y = y_t + .06, 
                   yend = y_t + .1), lwd = .5, 
               color = 'grey60') +  
  # center notch
  geom_segment(data = table_stats, 
               aes(x = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type'), 
                   xend = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type'),
                   y = y_t + .09, 
                   yend = y_t + .11), lwd = .5, 
               color = 'grey60') + 
  geom_text(data = table_stats, 
            aes(x = x*(Type == 'Not/lower type') + 
                  xend*(Type != 'Not/lower type'),
                y = y_t + .17, label = paste0(ifelse((Type == 'Not/lower type'), p_low, p_high)), size = 3), 
            color = 'grey30', size = 2.5) +
  labs(title = 'Average response, true and false sharing, control condition and pooled respondent',
       subtitle = 'Evaluation data'
       #,
       # caption = 'P-values represent statistical significance of treatment effect estimates.'
  ) +
  ylab('Estimate')

ggsave('../figures/any_sharing_by_covariates_te_comparisons.png', height = 7, width = 9.5)


p +
  stat_pointinterval(aes(x = label,
                         ydist = distributional::dist_normal(estimate, std.error)),
                     position = position_dodge(width=0.5),
                     .width = c(.66, .95, .99),
                     point_size = 0,
                     color = 'grey60') + 
  # flat bar
  geom_segment(data = table_stats0, 
               aes(x = x, xend = xend,
                   y = y + .1, 
                   yend = y + .1), lwd = .5, 
               color = 'grey60') + 
  # left leg
  geom_segment(data = table_stats0, 
               aes(x = x, xend = x,
                   y = y + .06, 
                   yend = y + .1), lwd = .5, 
               color = 'grey60') + 
  # right leg
  geom_segment(data = table_stats0, 
               aes(x = xend, xend = xend,
                   y = y + .06, 
                   yend = y + .1), lwd = .5, 
               color = 'grey60') +  
  # flat bar0
  geom_segment(data = table_stats0, 
               aes(x = x-.15, xend = x+.15,
                   y = y + .06, 
                   yend = y + .06), lwd = .5, 
               color = 'grey60') + 
  # left leg0
  geom_segment(data = table_stats0, 
               aes(x = x-.15, xend = x-.15,
                   y = y + .03, 
                   yend = y + .06), lwd = .5, 
               color = 'grey60') + 
  # right leg0
  geom_segment(data = table_stats0, 
               aes(x = x+.15, xend = x+.15,
                   y = y + .03, 
                   yend = y + .06), lwd = .5, 
               color = 'grey60') +  
  # flat bar1
  geom_segment(data = table_stats0, 
               aes(x = xend-.15, xend = xend+.15,
                   y = y + .06, 
                   yend = y + .06), lwd = .5, 
               color = 'grey60') + 
  # left leg1
  geom_segment(data = table_stats0, 
               aes(x = xend-.15, xend = xend-.15,
                   y = y + .03, 
                   yend = y + .06), lwd = .5, 
               color = 'grey60') + 
  # right leg1
  geom_segment(data = table_stats0, 
               aes(x = xend+.15, xend = xend+.15,
                   y = y + .03, 
                   yend = y + .06), lwd = .5, 
               color = 'grey60') +  
  # center notch
  geom_segment(data = table_stats0, 
               aes(x = 1.5, xend = 1.5,
                   y = y + .09, 
                   yend = y + .11), lwd = .5, 
               color = 'grey60') + 
  geom_text(data = table_stats0, 
            aes(x = 1.5,
                y = y + .17, label = paste0(p)), 
            color = 'grey30', size = 2.5) +
  labs(title = 'Average response, true and false sharing, control condition and pooled respondent',
       subtitle = 'Evaluation data'
       # ,
       # caption = 'P-values represent statistical significance of difference across covariate types in treatment effect estimates.'
  ) +
  ylab('Estimate') 

ggsave('../figures/any_sharing_by_covariates.png', height = 7, width = 9.5)


```

```{r pooled_respondent_any_false_on_covariates_figure, cache=TRUE}
p <- ggplot(table[which(table$`Stimuli type` == 'False'),], 
            aes(x = estimate, y = label, group = Condition, fill = Condition)) +
  stat_gradientinterval(
    aes(y = label, 
        xdist = distributional::dist_normal(estimate, std.error),),
    position=position_dodge(width=0.5),
    .width = 0, size = 0, height = 0.4,
    fill = cbPalette[7], color = cbPalette[7])  +
  geom_point(position = position_dodge(width=0.5), size = 4,
             shape = 21, color = cbPalette[7]) +
  facet_grid(rows = vars(`Covariate type`), scales = 'free_y', switch = 'y') +
  theme_minimal() + 
  coord_cartesian(xlim = c(0.3, 0.65)) + 
  scale_fill_manual(values =c('#FFFFFFFF', cbPalette[7])) +
  #scale_color_manual(values =c('#FFFFFFFF', cbPalette[7])) +
  theme(
    plot.title = element_text(hjust = 0.5),
    panel.background = element_rect(fill = 'white'),
    plot.background = element_rect(fill = 'white', color = 'white'),
    strip.text.y.left = element_text(angle = 0, hjust = 0, vjust = 1),
    strip.placement = "outside") +
  # scale_y_discrete(guide=guide_axis(n.dodge=2)) +
  ylab('Covariate types') + 
  xlab('Estimate')

p +
  geom_errorbar(aes(xmin = estimate - 1.96*std.error, xmax = estimate + 1.96*std.error), 
                width = 0.05, position=position_dodge(width=0.5), color = cbPalette[7]) +
  # flat bar
  geom_segment(data = table_stats[which(table_stats$`Stimuli type` == 'False'),], 
               aes(y = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type')-.125, 
                   yend = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type')+.125,
                   x = y_t + .075, 
                   xend = y_t + .075), lwd = .5, 
               color = 'grey60') + 
  # left leg
  geom_segment(data = table_stats[which(table_stats$`Stimuli type` == 'False'),], 
               aes(y = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type')-.125, 
                   yend = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type')-.125,
                   x = y_t + .035, 
                   xend = y_t + .075), lwd = .5, 
               color = 'grey60') + 
  # right leg
  geom_segment(data = table_stats[which(table_stats$`Stimuli type` == 'False'),], 
               aes(y = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type')+.125, 
                   yend = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type')+.125,
                   x = y_t + .035, 
                   xend = y_t + .075), lwd = .5, 
               color = 'grey60') +  
  # center notch
  geom_segment(data = table_stats[which(table_stats$`Stimuli type` == 'False'),], 
               aes(y = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type'), 
                   yend = x*(Type == 'Not/lower type') + 
                     xend*(Type != 'Not/lower type'),
                   x = y_t + .065, 
                   xend = y_t + .085), lwd = .5, 
               color = 'grey60') + 
  geom_text(data = table_stats[which(table_stats$`Stimuli type` == 'False'),], 
            aes(y = x*(Type == 'Not/lower type') + 
                  xend*(Type != 'Not/lower type'),
                x = y_t + .1, label = paste0(ifelse((Type == 'Not/lower type'), p_low, p_high))), 
            color = 'grey30', size = 3) +
  labs(title = 'Average response, false sharing, pooled respondent treatment and control condition',
       subtitle = 'Evaluation data'
       # ,
       # caption = 'P-values represent two-sided statistical significance of difference across covariate types in treatment effect estimates.'
  ) +
  ylab('Average intended stimuli shares on either channel') 

ggsave('../figures/any_sharing_by_covariates_te_comparisons_false.png', width = 8, height = 6)
```

### Rate
```{r RATE_accuracy_vs_facebook, cache=TRUE}

# Compute a prioritization based on estimated treatment effects.
test_idx <- ws_eval %in%c(4,5)
priority.cate <- -1 * predict(cf.priority, xs_eval[test_idx,])$predictions

# Estimate AUTOC on held out data.
cf.eval <- causal_forest(
  X = xs_eval[test_idx,], 
  Y = df_eval$post_false_prop[test_idx], 
  W = 1*(ws_eval[test_idx]==4), 
  W.hat = rowSums(probs_eval[test_idx, c(6), drop = FALSE])/rowSums(probs_eval[test_idx, c(6, 11)]),
  sample.weights = balwts_eval[test_idx],
  seed = 60637
)
rate <- rank_average_treatment_effect(cf.eval, priority.cate, 
                                      q = seq(0.02, 1, by = 0.02))
rate

ggplot(rate$TOC, aes(x = q, y = estimate)) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "grey60") + 
  geom_line() + 
  geom_ribbon(aes(ymax =  estimate + 1.96 *std.err, 
                  ymin =  estimate - 1.96 *std.err), alpha = .1, fill = cbPalette[7]) +
  theme_minimal() + 
  scale_x_continuous(limits = c(0.1, 1), breaks = seq(0.2, 1, 0.1)) + 
  coord_cartesian(ylim = c(-0.1, 0.05)) + 
  scale_y_continuous(breaks= seq(-0.1, 0.1, 0.05)) +
  theme(
    plot.title = element_text(hjust = 0.5),
    panel.background = element_rect(fill = 'white'),
    plot.background = element_rect(fill = 'white', color = 'white'),
    axis.title.y = element_text(vjust = 0.65), 
    panel.grid.minor.x = element_blank()) + 
  labs(title = "Targeting Operator Characteristic,\nAccuracy nudge vs. Facebook tips", 
       y = "Estimate", 
       x = "Proportion of the population assigned accuracy nudge as compared to Facebook tips"
       #, subtitle = "Shaded region shows 95% CI"
  )

ggsave('../figures/rate_accuracy_vs_facebook.png', height = 5, width = 8)

# Benefit at 40%
rate$TOC[which(rate$TOC$q == .4),]
```


## Additional
* ### Follow up on bonus stimuli [not coded yet]
+ Notes on analysis
+ Outcomes:
1. ) Why respondents chose to share or not share, hand-coded separately (question text: "In a few words, please say why you would or would not like to share this story on Facebook.")
+ Use evaluation data
+ Hypotheses
-  [ ] Compare responses on motivation for sharing for both true and false stimuli. i.e. compare responses on sharing true stimuli vs. sharing the correction of false stimuli; compare for each factor level separately

* ### Follow up questions 1-3 days after
+ Notes on analysis
+ Outcomes:
1. ) Sharing intentions as formalized in primary response function (Y =−Mb+0.5Tb, pre-analysis plan p. 17)
3. ) Sharing intentions for true/false stimuli separately (as proportion of stimuli of each type seen)
4. ) Sharing intentions for true/false stimuli and by timeline/messenger channel separately (as proportion of stimuli of each type seen)
+ Respondents should only see 1 true and 1 false stimuli
+ Following Broockman et al. (2017), we will analyze duration effects adjusting for differential response rate to the follow-up survey and the number of days since the respondent completed the survey
+ Use evaluation data
+ Hypotheses
-  Estimate mean response under each factor level separately

* ### Open response question for deliberation question [not coded yet]
+ Notes on analysis
+ Outcomes:
1. ) Why respondents chose to share or not share, hand-coded separately (question text: "In a few words, please say why you would or would not like to share this story on Facebook.")
+ Use evaluation data
+ Hypotheses
-  Compare responses on motivation for sharing for both true and false stimuli. i.e. compare responses on sharing true stimuli vs. sharing the correction of false stimuli


* ### Compare sample to Facebook and general pop
+ We will analyze how our sample compares to both the Facebook population and the general population in Kenya and Nigeria using Facebook’s advertising API data and nationally representative Afrobarometer surveys conducted in both countries.
+ Conduct separately for learning and evaluation 

* ### Aggregate number of times the associated Facebook post for each stimuli was shared 
+ (need to check Facebook timeline)


# Etc. 

Results reported inline that are not otherwise in tables. 
```{r results_control}
# UNDER CONTROL
# Difference in sharing of any true vs. false
x <- (aipw_scores_any_true_eval[,1] - aipw_scores_any_false_eval[,1])
# pvalue greater than
t.test(x, alternative = 'greater')$estimate
t.test(x, alternative = 'greater')$p.value
# pvalue two-sided
t.test(x)$p.value

# Difference in TRUE sharing of timeline vs. messenger
x <- (aipw_scores_timeline_true_eval[,1] - aipw_scores_send_true_eval[,1])
# pvalue greater than
t.test(x, alternative = 'greater')$estimate
t.test(x, alternative = 'greater')$p.value
# pvalue two-sided
t.test(x)$p.value

# Difference in FALSE sharing of timeline vs. messenger
x <- (aipw_scores_timeline_false_eval[,1] - aipw_scores_send_false_eval[,1])
# pvalue less than
t.test(x, alternative = 'less')$estimate
t.test(x, alternative = 'less')$p.value
# pvalue two-sided
t.test(x)$p.value
```

```{r results_heterogeneity_best}

# HETEROGENEITY IN BEST
# we achieve larger magnitude treatment effects in decreasing false sharing intentions through our contextual policy as compared to either the accuracy nudge or the Facebook tips treatments assigned uniformly

# accuracy
x <- (aipw_scores_any_false_eval[,6] - aipw_scores_send_false_eval[,4])
# pvalue less than
t.test(x, alternative = 'less')$estimate
t.test(x, alternative = 'less')$p.value
# pvalue two-sided
t.test(x)$p.value

# fb
x <- (aipw_scores_any_false_eval[,6] - aipw_scores_send_false_eval[,5])
# pvalue less than
t.test(x, alternative = 'less')$estimate
t.test(x, alternative = 'less')$p.value
# pvalue two-sided
t.test(x)$p.value
```


```{r results_channel}

# CHANNEL SHARING
# the Facebook tips treatment has larger effects on mitigating false sharing intentions relative to the accuracy nudge
x <- (aipw_scores_any_false_eval[,5] - aipw_scores_any_false_eval[,4])
# pvalue greater than
t.test(x, alternative = 'greater')$estimate
t.test(x, alternative = 'greater')$p.value
# pvalue less than
t.test(x, alternative = 'less')$estimate
t.test(x, alternative = 'less')$p.value
# pvalue two-sided
t.test(x)$p.value

# and also true
x <- (aipw_scores_any_true_eval[,5] - aipw_scores_any_true_eval[,4])
# pvalue greater than
t.test(x, alternative = 'greater')$estimate
t.test(x, alternative = 'greater')$p.value
# pvalue less than
t.test(x, alternative = 'less')$estimate
t.test(x, alternative = 'less')$p.value
# pvalue two-sided
t.test(x)$p.value

# combined
x <- (aipw_scores[,5] - aipw_scores[,4])
# pvalue greater than
t.test(x, alternative = 'greater')$estimate
t.test(x, alternative = 'greater')$p.value
# pvalue less than
t.test(x, alternative = 'less')$estimate
t.test(x, alternative = 'less')$p.value
# pvalue two-sided
t.test(x)$p.value


# The Facebook tips treatment and the accuracy nudge are both effective at moving false sharing intentions on the timeline
# fb
x <- aipw_scores_timeline_false_eval[,5]
# pvalue less than
t.test(x, alternative = 'less')$estimate
t.test(x, alternative = 'less')$p.value
# pvalue two-sided
t.test(x)$p.value

# accuracy
x <- aipw_scores_timeline_false_eval[,4]
# pvalue less than
t.test(x, alternative = 'less')$estimate
t.test(x, alternative = 'less')$p.value
# pvalue two-sided
t.test(x)$p.value

# fb
x <- aipw_scores_send_false_eval[,5]
# pvalue less than
t.test(x, alternative = 'less')$estimate
t.test(x, alternative = 'less')$p.value
# pvalue two-sided
t.test(x)$p.value

# accuracy
x <- aipw_scores_send_false_eval[,4]
# pvalue less than
t.test(x, alternative = 'less')$estimate
t.test(x, alternative = 'less')$p.value
# pvalue two-sided
t.test(x)$p.value


# The Facebook tips treatment is relatively more effective at also moving false sharing intentions on Messenger 
x <- (aipw_scores_send_false_eval[,5] - aipw_scores_send_false_eval[,4])
# pvalue greater than
t.test(x, alternative = 'greater')$estimate
t.test(x, alternative = 'greater')$p.value
# pvalue less than
t.test(x, alternative = 'less')$estimate
t.test(x, alternative = 'less')$p.value
# pvalue two-sided
t.test(x)$p.value
```

```{r results_heterogeneity_overall}

# HETEROGENEITY OVERALL
# The largest treatment effects on false sharing intentions were for users with below median digital literacy and below median scientific knowledge. For these users, the Facebook tips treatment was more effective than the accuracy nudge

covariate <- 'dli'
adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
x <- aipw_scores_any_false_eval[,5]-aipw_scores_any_false_eval[,4]

fmla0 <- formula(paste0('x ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
(ols0 <- tidy(lm_robust(fmla0, data=df_eval)))

covariate <- 'science'
adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
fmla0 <- formula(paste0('x ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
(ols0 <- tidy(lm_robust(fmla0, data=df_eval)))

# For users with below median digital literacy and below median scientific knowledge, treatment effects under Facebook tips were driven by relatively larger effects on private sharing on Messenger as compared to public sharing on their timelines

# for fb
covariate <- 'dli'
adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
x <- aipw_scores_timeline_false_eval[,5]-aipw_scores_send_false_eval[,5]

fmla0 <- formula(paste0('x ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
(ols0 <- tidy(lm_robust(fmla0, data=df_eval)))

covariate <- 'science'
adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
fmla0 <- formula(paste0('x ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
(ols0 <- tidy(lm_robust(fmla0, data=df_eval)))

# whereas for the accuracy nudge, effects on timeline as compared to messenger sharing are comparable for these groups 
covariate <- 'dli'
adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
x <- aipw_scores_timeline_false_eval[,4]-aipw_scores_send_false_eval[,4]

fmla0 <- formula(paste0('x ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
(ols0 <- tidy(lm_robust(fmla0, data=df_eval)))

covariate <- 'science'
adjustment <--1e-5*c(2*(median(df_eval[,covariate])==quantile(df_eval[,covariate], 1))-1)
fmla0 <- formula(paste0('x ~  factor(',covariate, '> median(df_eval$', covariate, ') +', adjustment,') -1'))
(ols0 <- tidy(lm_robust(fmla0, data=df_eval)))


```


```{r old_optimal_policy_prediction, cache = TRUE, eval = FALSE}
W_forest <- readRDS('objects/policy_objects/W_forest.rds')
multi_forest <- readRDS('objects/policy_objects/multi_forest.rds')
Y_forest <- readRDS('objects/policy_objects/Y_forest.rds')

# 2a. Predict m(x) = E[Y|X]
Y_hat_test <- predict(Y_forest, xs_h_new)$predictions[,1]
# 2b. Predict W.hat = E[W|X] 
W_hat_test <- predict(W_forest, xs_h_new)$predictions
# 2c. Predict baseline, E[Y(k) | X = x], and tau_k(X) = E[Y(k') - Y(k) | X = x]; 
# here accuracy is baseline, 
tau_hat_test <- predict(multi_forest, xs_h_new)$predictions[,,]
Y_hat_baseline_test <- Y_hat_test - rowSums(W_hat_test[, -1, drop = FALSE] * tau_hat_test)

# 3. combine to get mu_k(X) on test data (and re-order)
muk_test <- cbind(Y_hat_baseline_test, Y_hat_baseline_test + tau_hat_test)[,c(2, 1, 3:8)]
names(muk_test) <- WR_levels


levels_implement <- c(2,4,7,8)
wopt_multi_forest <- WH_idx[[1]][levels_implement[apply(
  muk_test[,levels_implement], 1, which.max
)]]

optimal_old <- case_when(wopt_multi_forest == 6 ~ 4,
                         wopt_multi_forest == 8 ~ 6, #8/12 collapsed
                         wopt_multi_forest == 11 ~ 5,
                         wopt_multi_forest == 12 ~ 6))

```


### Covariates by optimal policy
```{r covariates_in_optimal}

.df <- df_eval %>% 
  mutate(leaf = optimal_assignment) %>% 
  group_by(leaf) %>% 
  mutate(leaf = factor(paste0(leaf, "\n ", "(n = ", n(), ")"))) %>% 
  as.data.frame() 

covs_of_interest <- c('age', 'dli', 'male', 'pol', 'science')
plot_covariate_means_by_group(
  .df,
  .title = "Covariate averages by optimal policy assignment",
  rowvars = covs_of_interest)

pvals <- sapply(covs_of_interest, function(x){
  cov <- x
  pval <- signif(lm_robust(as.formula(paste0(x, ' ~ leaf')), data = .df)$p.value[[2]], digits=3)
  c(cov, pval)
} )
kable(pvals)

ggsave('../figures/covariate_optimal.png', 
       width = 6.25, height = 4)


plot_covariate_means_by_group(
  .df,
  .title = "Covariate averages by optimal policy assignment",
  n_top = 22)

ggsave('../figures/covariate_optimal_full.png', 
       width = 6.25, height = 7)

```

#### P-value tables
```{r any_eval_pvalues, cache = TRUE}
control_scores_any_true <- aipw_scores_any_true_eval[, 1]
h_factcheck_scores_any_true <- aipw_scores_any_true_eval[, 2] 
h_related_scores_any_true <- aipw_scores_any_true_eval[, 3]
r_accuracy_scores_any_true <- aipw_scores_any_true_eval[, 4]
r_tips_facebook_scores_any_true <- aipw_scores_any_true_eval[, 5]
r_optimal_scores_any_true <- aipw_scores_any_true_eval[, 6]

control_scores_any_false <- aipw_scores_any_false_eval[, 1]
h_factcheck_scores_any_false <- aipw_scores_any_false_eval[, 2] 
h_related_scores_any_false <- aipw_scores_any_false_eval[, 3]
r_accuracy_scores_any_false <- aipw_scores_any_false_eval[, 4]
r_tips_facebook_scores_any_false <- aipw_scores_any_false_eval[, 5]
r_optimal_scores_any_false <- aipw_scores_any_false_eval[, 6]


# TRUE
scoresl <- list(control_scores_any_true, 
                control_scores_any_true+h_factcheck_scores_any_true, 
                control_scores_any_true+h_related_scores_any_true, 
                control_scores_any_true+r_accuracy_scores_any_true, 
                control_scores_any_true+r_tips_facebook_scores_any_true, 
                control_scores_any_true+r_optimal_scores_any_true)

scoresln <- c('control_scores', 
              'h_factcheck_scores', 
              'h_related_scores', 
              'r_accuracy_scores', 
              'r_tips_facebook_scores', 
              'r_optimal_scores')

names(scoresl) <- scoresln
# p-values for two-sided hypotheses
scores_map <- expand.grid(scoresln, scoresln, stringsAsFactors = FALSE)

outmat <- matrix(mapply(pval, scores_map[,1], scores_map[,2]), ncol = length(scoresln), byrow = FALSE)
colnames(outmat) <- rownames(outmat) <- treatment_levels


kable(round(outmat * upper.tri(outmat),3), format="html", digits=3, 
      caption='P-values for test that row == column, any true share', 
      escape = FALSE,
      row.names = TRUE) %>%
  kable_styling(bootstrap_options=c("condensed", "responsive"),
                full_width=FALSE)

kable(round(outmat * upper.tri(outmat)*1,3), format="latex", digits=3, 
      caption='P-values for test that row == column, any true share', 
      escape = FALSE,
      row.names = TRUE) %>%
  kable_styling(bootstrap_options=c("condensed", "responsive"),
                full_width=FALSE) %>% 
  save_kable(file = '../tables/evaluated_pvals_twosided_any_true.tex')


# p-values for hypothesis that y is greater than x
outmat_ge <- matrix(mapply(pval_ge, scores_map[,1], scores_map[,2]), ncol = length(scoresln), byrow = FALSE)
colnames(outmat_ge) <- rownames(outmat_ge) <- treatment_levels

kable(round(t(outmat_ge), 3), format="html", digits=3, 
      caption='P-values for test that row < column, any true share', 
      escape = FALSE,
      row.names = TRUE) %>%
  kable_styling(bootstrap_options=c("condensed", "responsive"),
                full_width=FALSE)

kable(round(t(outmat_ge), 3), format="latex", digits=3, 
      caption='P-values for test that row < column, main response function', 
      escape = FALSE,
      row.names = TRUE) %>%
  kable_styling(bootstrap_options=c("condensed", "responsive"),
                full_width=FALSE) %>% 
  save_kable(file = '../tables/evaluated_pvals_onesided_any_true.tex')


# FALSE
scoresl <- list(control_scores_any_false, 
                control_scores_any_false+h_factcheck_scores_any_false, 
                control_scores_any_false+h_related_scores_any_false, 
                control_scores_any_false+r_accuracy_scores_any_false, 
                control_scores_any_false+r_tips_facebook_scores_any_false, 
                control_scores_any_false+r_optimal_scores_any_false)

scoresln <- c('control_scores', 
              'h_factcheck_scores', 
              'h_related_scores', 
              'r_accuracy_scores', 
              'r_tips_facebook_scores', 
              'r_optimal_scores')

names(scoresl) <- scoresln
# p-values for two-sided hypotheses
scores_map <- expand.grid(scoresln, scoresln, stringsAsFactors = FALSE)

outmat <- matrix(mapply(pval, scores_map[,1], scores_map[,2]), ncol = length(scoresln), byrow = FALSE)
colnames(outmat) <- rownames(outmat) <- treatment_levels


kable(round(outmat * upper.tri(outmat),3), format="html", digits=3, 
      caption='P-values for test that row == column, any false share', 
      escape = FALSE,
      row.names = TRUE) %>%
  kable_styling(bootstrap_options=c("condensed", "responsive"),
                full_width=FALSE)

kable(round(outmat * upper.tri(outmat)*1,3), format="latex", digits=3, 
      caption='P-values for test that row == column, any false share', 
      escape = FALSE,
      row.names = TRUE) %>%
  kable_styling(bootstrap_options=c("condensed", "responsive"),
                full_width=FALSE) %>% 
  save_kable(file = '../tables/evaluated_pvals_twosided_any_false.tex')


# p-values for hypothesis that y is greater than x
outmat_ge <- matrix(mapply(pval_ge, scores_map[,1], scores_map[,2]), ncol = length(scoresln), byrow = FALSE)
colnames(outmat_ge) <- rownames(outmat_ge) <- treatment_levels

kable(round(t(outmat_ge), 3), format="html", digits=3, 
      caption='P-values for test that row < column, any false share', 
      escape = FALSE,
      row.names = TRUE) %>%
  kable_styling(bootstrap_options=c("condensed", "responsive"),
                full_width=FALSE)

kable(round(t(outmat_ge), 3), format="latex", digits=3, 
      caption='P-values for test that row < column, main response function', 
      escape = FALSE,
      row.names = TRUE) %>%
  kable_styling(bootstrap_options=c("condensed", "responsive"),
                full_width=FALSE) %>% 
  save_kable(file = '../tables/evaluated_pvals_onesided_any_false.tex')
```



```{r load_cached, eval = FALSE}
qwraps2::lazyload_cache_dir(path = "misinformation_replication_cache/html")
source('utils.R')
```

